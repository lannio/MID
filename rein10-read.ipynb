{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "# print\n",
    "## param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "import yaml\n",
    "import argparse\n",
    "import dill\n",
    "import pdb\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils as utils\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions.multivariate_normal as torchdist\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "from torch.utils import data\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Module, Parameter, ModuleList, Linear\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from tqdm.auto import tqdm\n",
    "from easydict import EasyDict\n",
    "\n",
    "import datetime, shutil, argparse, logging, sys\n",
    " \n",
    "from dataset.preprocessing import get_node_timestep_data, collate \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--run_file',type=str, default='rein10.py')\n",
    "    parser.add_argument('--config_file',type=str, default='./configs/baseline.yaml')\n",
    "\n",
    "    parser.add_argument('--folder_date',type=str, default='read')\n",
    "    parser.add_argument('--dataset',type=str, default='eth')\n",
    "    parser.add_argument('--exp',type=str, default='rein10-test1')\n",
    "\n",
    "    parser.add_argument('--read', default=True, type=bool)\n",
    "    parser.add_argument('--model',type=str, default='/home/yaoliu/scratch/experiment/rein/test-0808-02/eth/baseline/95_model.pt')\n",
    "\n",
    "    parser.add_argument(\"--batch_size\", default=256, type=int)\n",
    "    parser.add_argument(\"--eval_batch_size\", default=256, type=int)\n",
    "    parser.add_argument('--shuffle', default=True, type=bool)\n",
    "    parser.add_argument(\"--eval_every\", default=10, type=int)\n",
    "\n",
    "    parser.add_argument(\"--num_steps\", default=100, type=int)\n",
    "    parser.add_argument(\"--num_ddim\", default=10, type=int)\n",
    "    parser.add_argument(\"--ddim_eta\", default=0.0, type=float)\n",
    "    parser.add_argument(\"--clip_denoised\", default=False, type=bool)\n",
    "\n",
    "    parser.add_argument(\"--loss_diffusion_rate\", default=10., type=float)\n",
    "    parser.add_argument(\"--loss_gau_rate\", default=10000., type=float)\n",
    "    parser.add_argument(\"--loss_mean_rate\", default=10., type=float)\n",
    "    parser.add_argument(\"--loss_left_gau\", default=10., type=float)\n",
    "    parser.add_argument(\"--lr\", default=0.001, type=float)\n",
    "    parser.add_argument(\"--lr2\", default=0.001, type=float)\n",
    "\n",
    "    parser.add_argument(\"--diffusion_sample_num\", default=1, type=int)\n",
    "    parser.add_argument(\"--point_dim\", default=2, type=int)\n",
    "    parser.add_argument(\"--pred_length\", default=1, type=int)\n",
    "    parser.add_argument(\"--end_list\", default=20, type=int)\n",
    "    parser.add_argument(\"--sample\", default=20, type=int)\n",
    "\n",
    "    parser.add_argument(\"--device\", default=7, type=int)\n",
    "    parser.add_argument(\"--isseed\", default=True, type=bool)\n",
    "    parser.add_argument('--seed', default=113, type=int)\n",
    "\n",
    "    parser.add_argument('--gamma', default=0.95, type=float)\n",
    "    parser.add_argument(\"--epochs\", default=100, type=int)\n",
    "    parser.add_argument(\"--augment\", default=True, type=bool)\n",
    "\n",
    "    parser.add_argument('--data_dir',type=str, default='./processed_data_new')    \n",
    "    parser.add_argument('--gpu_deterministic', default=False, type=bool, help='set cudnn in deterministic mode (slow)')\n",
    "\n",
    "\n",
    "    return parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-11 14:52:00,413 : Namespace(augment=True, batch_size=256, clip_denoised=False, config_file='./configs/baseline.yaml', data_dir='./processed_data_new', dataset='eth', ddim_eta=0.0, device=7, diffusion_sample_num=1, end_list=20, epochs=100, eval_batch_size=256, eval_every=10, exp='rein10-test1', folder_date='read', gamma=0.95, gpu_deterministic=False, isseed=True, loss_diffusion_rate=10.0, loss_gau_rate=10000.0, loss_left_gau=10.0, loss_mean_rate=10.0, lr=0.001, lr2=0.001, model='/home/yaoliu/scratch/experiment/rein/test-0808-02/eth/baseline/95_model.pt', num_ddim=10, num_steps=100, point_dim=2, pred_length=1, read=True, run_file='rein10.py', sample=20, seed=113, shuffle=True)\n",
      "2023-08-11 14:52:00,415 : --------build--------\n",
      "2023-08-11 14:52:00,416 : ----dataset begin----\n",
      "2023-08-11 14:52:00,416 : train_data_path: ./processed_data_new/eth_train.pkl\n",
      "2023-08-11 14:52:00,418 : eval_data_path: ./processed_data_new/eth_test.pkl\n",
      "2023-08-11 14:52:03,940 : ----dataset end----\n",
      "2023-08-11 14:52:03,943 : ----model begin----\n"
     ]
    }
   ],
   "source": [
    "def get_traj_hypers():\n",
    "    hypers = { \n",
    "    'state':\n",
    "        {'PEDESTRIAN':\n",
    "            {'position': ['x', 'y'],\n",
    "             'velocity': ['x', 'y'],\n",
    "             'acceleration': ['x', 'y']\n",
    "            }\n",
    "        },\n",
    "    'pred_state': {'PEDESTRIAN': {'velocity': ['x', 'y']}},\n",
    "    'edge_encoding': True,\n",
    "    'edge_addition_filter': [0.25, 0.5, 0.75, 1.0],\n",
    "    'edge_removal_filter': [1.0, 0.0],\n",
    "    'dynamic_edges': 'yes',\n",
    "    'incl_robot_node': False,\n",
    "    'node_freq_mult_train': False,\n",
    "    'node_freq_mult_eval': False,\n",
    "    'scene_freq_mult_train': False,\n",
    "    'scene_freq_mult_eval': False,\n",
    "    'scene_freq_mult_viz': False,\n",
    "    'use_map_encoding': False,\n",
    "    }\n",
    "    return hypers\n",
    "\n",
    "\n",
    "# common function\n",
    "def set_gpu(gpu):\n",
    "    torch.cuda.set_device('cuda:{}'.format(gpu))\n",
    "\n",
    "def set_cuda(deterministic=True):\n",
    "    if torch.cuda.is_available():\n",
    "        if not deterministic:\n",
    "            torch.backends.cudnn.deterministic = False\n",
    "            torch.backends.cudnn.benchmark = True\n",
    "        else:\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "            torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def get_output_dir(folder,dataset,exp):\n",
    "    output_dir = os.path.join('/home/yaoliu/scratch/experiment/rein/' + folder, dataset, exp)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    return output_dir\n",
    "\n",
    "def setup_logging(name, output_dir, console=True):\n",
    "    log_format = logging.Formatter(\"%(asctime)s : %(message)s\")\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.handlers = []\n",
    "    output_file = os.path.join(output_dir, 'output.log')\n",
    "    file_handler = logging.FileHandler(output_file)\n",
    "    file_handler.setFormatter(log_format)\n",
    "    logger.addHandler(file_handler)\n",
    "    if console:\n",
    "        console_handler = logging.StreamHandler(sys.stdout)\n",
    "        console_handler.setFormatter(log_format)\n",
    "        logger.addHandler(console_handler)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    return logger\n",
    "\n",
    "def set_seed(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def copy_source(file, output_dir):\n",
    "    shutil.copyfile(file, os.path.join(output_dir, os.path.basename(file)))\n",
    "\n",
    "def restore(data):\n",
    "    \"\"\"\n",
    "    In case we dilled some structures to share between multiple process this function will restore them.\n",
    "    If the data input are not bytes we assume it was not dilled in the first place\n",
    "\n",
    "    :param data: Possibly dilled data structure\n",
    "    :return: Un-dilled data structure\n",
    "    \"\"\"\n",
    "    if type(data) is bytes:\n",
    "        return dill.loads(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "# data function\n",
    "class EnvironmentDataset(object):\n",
    "    def __init__(self, env, state, pred_state, node_freq_mult, scene_freq_mult, hyperparams, **kwargs):\n",
    "        self.env = env\n",
    "        self.state = state\n",
    "        self.pred_state = pred_state\n",
    "        self.hyperparams = hyperparams\n",
    "        self.max_ht = 7 # 7\n",
    "        self.max_ft = 12\n",
    "        self.node_type_datasets = list() # 1-->1670\n",
    "        self._augment = False\n",
    "        for node_type in env.NodeType:\n",
    "            if node_type not in hyperparams['pred_state']:\n",
    "                continue\n",
    "            self.node_type_datasets.append(NodeTypeDataset(env, node_type, state, pred_state, node_freq_mult,\n",
    "                                                           scene_freq_mult, hyperparams, **kwargs))\n",
    "\n",
    "    @property\n",
    "    def augment(self):\n",
    "        return self._augment\n",
    "\n",
    "    @augment.setter\n",
    "    def augment(self, value):\n",
    "        self._augment = value\n",
    "        for node_type_dataset in self.node_type_datasets:\n",
    "            node_type_dataset.augment = value\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.node_type_datasets)\n",
    "\n",
    "\n",
    "class NodeTypeDataset(data.Dataset):\n",
    "    def __init__(self, env, node_type, state, pred_state, node_freq_mult,\n",
    "                 scene_freq_mult, hyperparams, augment=False, **kwargs):\n",
    "        self.env = env\n",
    "        self.state = state\n",
    "        '''\n",
    "        {'PEDESTRIAN': {'position': ['x', 'y'],\n",
    "        'velocity': ['x', 'y'],\n",
    "        'acceleration': ['x', 'y']}}\n",
    "        '''\n",
    "        self.pred_state = pred_state # {'PEDESTRIAN': {'velocity': ['x', 'y']}}\n",
    "        self.hyperparams = hyperparams\n",
    "        self.max_ht = 7 # 7\n",
    "        self.max_ft = 12 #12\n",
    "\n",
    "        self.augment = augment\n",
    "\n",
    "        self.node_type = node_type # PEDESTRIAN\n",
    "        self.index = self.index_env(node_freq_mult, scene_freq_mult, **kwargs)\n",
    "        self.len = len(self.index) # 1670\n",
    "        self.edge_types = [edge_type for edge_type in env.get_edge_types() if edge_type[0] is node_type] # [(PEDESTRIAN, PEDESTRIAN)]\n",
    "\n",
    "    def index_env(self, node_freq_mult, scene_freq_mult, **kwargs): # False False\n",
    "        index = list()\n",
    "        for scene in self.env.scenes:\n",
    "            present_node_dict = scene.present_nodes(np.arange(0, scene.timesteps), type=self.node_type, **kwargs)\n",
    "            for t, nodes in present_node_dict.items():\n",
    "                for node in nodes:\n",
    "                    index += [(scene, t, node)] *\\\n",
    "                             (scene.frequency_multiplier if scene_freq_mult else 1) *\\\n",
    "                             (node.frequency_multiplier if node_freq_mult else 1)\n",
    "\n",
    "        return index\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        (scene, t, node) = self.index[i]\n",
    "\n",
    "        if self.augment:\n",
    "            scene = scene.augment()\n",
    "            node = scene.get_node_by_id(node.id)\n",
    "\n",
    "        return get_node_timestep_data(self.env, scene, t, node, self.state, self.pred_state,\n",
    "                                      self.edge_types, self.max_ht, self.max_ft, self.hyperparams)\n",
    "    \n",
    "\n",
    "\n",
    "class VarianceSchedule(Module):\n",
    "\n",
    "    def __init__(self, num_steps, mode='linear',beta_1=1e-4, beta_T=5e-2, cosine_s=8e-3):\n",
    "        '''\n",
    "            num_steps=100,\n",
    "            beta_T=5e-2,\n",
    "            mode='linear'\n",
    "        '''\n",
    "        super().__init__()\n",
    "        assert mode in ('linear', 'cosine')\n",
    "        self.num_steps = num_steps # 100\n",
    "\n",
    "        self.beta_1 = beta_1 # 1e-4\n",
    "        self.beta_T = beta_T # 5e-2\n",
    "        self.mode = mode # 'linear'\n",
    "\n",
    "        if mode == 'linear':\n",
    "            betas = torch.linspace(beta_1, beta_T, steps=num_steps)\n",
    "        elif mode == 'cosine':\n",
    "            timesteps = (\n",
    "            torch.arange(num_steps + 1) / num_steps + cosine_s\n",
    "            )\n",
    "            alphas = timesteps / (1 + cosine_s) * math.pi / 2\n",
    "            alphas = torch.cos(alphas).pow(2)\n",
    "            alphas = alphas / alphas[0]\n",
    "            betas = 1 - alphas[1:] / alphas[:-1]\n",
    "            betas = betas.clamp(max=0.999)\n",
    "\n",
    "        betas = torch.cat([torch.zeros([1]), betas], dim=0)     # Padding\n",
    "\n",
    "        alphas = 1 - betas\n",
    "        log_alphas = torch.log(alphas)\n",
    "        for i in range(1, log_alphas.size(0)):  # 1 to T\n",
    "            log_alphas[i] += log_alphas[i - 1]\n",
    "        alpha_bars = log_alphas.exp()\n",
    "\n",
    "        sigmas_flex = torch.sqrt(betas)\n",
    "        sigmas_inflex = torch.zeros_like(sigmas_flex)\n",
    "        for i in range(1, sigmas_flex.size(0)):\n",
    "            sigmas_inflex[i] = ((1 - alpha_bars[i-1]) / (1 - alpha_bars[i])) * betas[i]\n",
    "        sigmas_inflex = torch.sqrt(sigmas_inflex)\n",
    "\n",
    "        self.register_buffer('betas', betas)\n",
    "        self.register_buffer('alphas', alphas)\n",
    "        self.register_buffer('alpha_bars', alpha_bars)\n",
    "        self.register_buffer('sigmas_flex', sigmas_flex)\n",
    "        self.register_buffer('sigmas_inflex', sigmas_inflex)\n",
    "\n",
    "    def uniform_sample_t(self, batch_size):\n",
    "        ts = np.random.choice(np.arange(1, self.num_steps+1), batch_size)\n",
    "        return ts.tolist()\n",
    "\n",
    "    def get_sigmas(self, t, flexibility):\n",
    "        assert 0 <= flexibility and flexibility <= 1\n",
    "        sigmas = self.sigmas_flex[t] * flexibility + self.sigmas_inflex[t] * (1 - flexibility)\n",
    "        return sigmas\n",
    "    \n",
    "\n",
    "\n",
    "def extract(a, t, x_shape):\n",
    "    batch_size = t.shape[0]\n",
    "    out = a.to(t.device).gather(0, t).float()\n",
    "    out = out.reshape(batch_size, *((1,) * (len(x_shape) - 1)))\n",
    "    return out\n",
    "\n",
    "def get_select_fde(selected_end,end):\n",
    "    # fde=torch.mean(torch.norm((selected_end.view(256, -1, 2) - end.view(256,-1, 2)),dim=2,dim=[0])\n",
    "    fde=torch.mean((torch.norm((selected_end.view(256, -1, 2) - end.view(256,-1, 2)),dim=2)),dim=0)\n",
    "    # fde = F.mse_loss(end.contiguous().view(-1, 2), selected_end.contiguous().view(-1, 2), reduction='mean')\n",
    "    return fde\n",
    "\n",
    "def get_pred_loss(pred, selected_end, gt):\n",
    "    # bs,12,5 bs,1,2, bs,12,2\n",
    "    sx = torch.exp(pred[:, :, 2])  # sx\n",
    "    sy = torch.exp(pred[:, :, 3])  # sy\n",
    "    corr = torch.tanh(pred[:, :, 4])  # corr\n",
    "\n",
    "    cov = torch.zeros(pred.shape[0], pred.shape[1], 2, 2).to('cuda')\n",
    "    cov[:, :, 0, 0] = sx * sx\n",
    "    cov[:, :, 0, 1] = corr * sx * sy\n",
    "    cov[:, :, 1, 0] = corr * sx * sy\n",
    "    cov[:, :, 1, 1] = sy * sy\n",
    "    mean = pred[:, :, 0:2]\n",
    "    mvn = torch.distributions.multivariate_normal.MultivariateNormal(mean, cov)\n",
    "    loss_gt = - mvn.log_prob(gt).sum()\n",
    "    loss_mean = F.mse_loss(mean[:,-1,:].contiguous().view(-1, 2), selected_end.contiguous().view(-1, 2), reduction='mean')\n",
    "    # loss=loss_gt/args.loss_gau_rate + loss_mean/args.loss_mean_rate\n",
    "\n",
    "    return loss_gt, loss_mean\n",
    "\n",
    "def get_pred_de(pred, gt):\n",
    "    predlist=len(pred)\n",
    "    kstep_V_pred_ls = []\n",
    "    gt = gt.permute(1,0,2)*0.4\n",
    "    pred = pred.permute(1,0,2)\n",
    "\n",
    "    sx = torch.exp(pred[:, :, 2])  # sx\n",
    "    sy = torch.exp(pred[:, :, 3])  # sy\n",
    "    corr = torch.tanh(pred[:, :, 4])  # corr\n",
    "\n",
    "    cov = torch.zeros(pred.shape[0], pred.shape[1], 2, 2).to('cuda')\n",
    "    cov[:, :, 0, 0] = sx * sx\n",
    "    cov[:, :, 0, 1] = corr * sx * sy\n",
    "    cov[:, :, 1, 0] = corr * sx * sy\n",
    "    cov[:, :, 1, 1] = sy * sy\n",
    "    mean = pred[:, :, 0:2]\n",
    "    mvn = torch.distributions.multivariate_normal.MultivariateNormal(mean, cov)\n",
    "\n",
    "    KSTEPS=args.sample\n",
    "    for i in range(KSTEPS-1):\n",
    "        kstep_V_pred_ls.append(torch.cumsum((mvn.sample()*0.4), dim=0))  # cat [12, num_person, 2]\n",
    "    kstep_V_pred_ls.append(torch.cumsum(mean*0.4, dim=0))\n",
    "    kstep_V_pred_ls = torch.stack(kstep_V_pred_ls, dim=0) # [KSTEPS, 12, num_person, 2]\n",
    "\n",
    "    # kstep_V_pred = np.concatenate([traj for traj in kstep_V_pred_ls], axis=1) # [12, KSTEPS * num_person, 2]\n",
    "\n",
    "    \"\"\"end of sampling\"\"\"\n",
    "\n",
    "    V_y_rel_to_abs =  torch.cumsum((gt), dim=0) # [12, num_person, 2] speed???)\n",
    "\n",
    "    ade=torch.mean(torch.min(torch.norm((kstep_V_pred_ls - V_y_rel_to_abs),dim=3),dim=0)[0],dim=[0,1])\n",
    "    fde=torch.mean(torch.min(torch.norm((kstep_V_pred_ls - V_y_rel_to_abs)[:,-1,:,:],dim=2),dim=0)[0],dim=[0])\n",
    "    return ade,fde\n",
    "\n",
    "\n",
    "\n",
    "def find_end(gauss_param_tensor, coordinates_list):\n",
    "    gauss_param_tensor = gauss_param_tensor[:,0,:]\n",
    "\n",
    "    sx = torch.exp(gauss_param_tensor[:, 2])  # sx\n",
    "    sy = torch.exp(gauss_param_tensor[:, 3])  # sy\n",
    "    corr = torch.tanh(gauss_param_tensor[:, 4])  # corr\n",
    "    cov = torch.zeros(gauss_param_tensor.shape[0], 2, 2).to('cuda')\n",
    "    cov[:, 0, 0] = sx * sx\n",
    "    cov[:, 0, 1] = corr * sx * sy\n",
    "    cov[:, 1, 0] = corr * sx * sy\n",
    "    cov[:, 1, 1] = sy * sy\n",
    "    mean = gauss_param_tensor[:, 0:2] # bs,1,2\n",
    "    # 创建MultivariateNormal分布对象\n",
    "    gauss_distribution = MultivariateNormal(mean, cov)\n",
    "\n",
    "    # 用来存储每个item的采样概率\n",
    "    sampling_probs = []\n",
    "\n",
    "    # 计算每个item与第一个tensor的采样概率\n",
    "    for coordinates_tensor in coordinates_list:\n",
    "\n",
    "        # 计算该item在第一个分布下的log概率之和\n",
    "        log_prob_sum = gauss_distribution.log_prob(coordinates_tensor).sum()\n",
    "\n",
    "        # 将采样概率存储到列表中\n",
    "        sampling_probs.append(log_prob_sum)\n",
    "\n",
    "    # 找到具有最大采样概率的item的索引\n",
    "    max_prob_index = torch.argmax(torch.tensor(sampling_probs))\n",
    "\n",
    "    # 选取最有可能是由第一个tensor采样得到的item\n",
    "    selected_tensor = coordinates_list[max_prob_index]\n",
    "    selected_sampling_probs= sampling_probs[max_prob_index]\n",
    "\n",
    "    return selected_tensor, selected_sampling_probs\n",
    "\n",
    "\n",
    "# main args\n",
    "args = parse_args()\n",
    "hyperparams = get_traj_hypers()\n",
    "\n",
    "\n",
    "# main output-log  \n",
    "\n",
    "output_dir = get_output_dir(args.folder_date, args.dataset, args.exp)\n",
    "copy_source(args.run_file, output_dir)\n",
    "copy_source(args.config_file, output_dir)\n",
    "\n",
    "\n",
    "set_gpu(args.device)\n",
    "set_cuda(deterministic=args.gpu_deterministic)\n",
    "if (args.isseed):\n",
    "    set_seed(args.seed)\n",
    "\n",
    "logger = setup_logging('job{}'.format(0), output_dir, console=True)\n",
    "logger.info(args)\n",
    "\n",
    "\n",
    "logger.info(\"--------build--------\")\n",
    "\n",
    "\n",
    "# data\n",
    "\n",
    "logger.info(\"----dataset begin----\")\n",
    "train_data_path = osp.join(args.data_dir,args.dataset + \"_train.pkl\")\n",
    "eval_data_path = osp.join(args.data_dir,args.dataset + \"_test.pkl\")\n",
    "logger.info(\"train_data_path: \"+ train_data_path)\n",
    "logger.info(\"eval_data_path: \"+ eval_data_path)\n",
    "\n",
    "train_scenes = []\n",
    "with open(train_data_path, 'rb') as f:\n",
    "    train_env = dill.load(f, encoding='latin1')\n",
    "train_scenes = train_env.scenes\n",
    "train_dataset = EnvironmentDataset(train_env,\n",
    "                                hyperparams['state'],\n",
    "                                hyperparams['pred_state'],\n",
    "                                scene_freq_mult=False,\n",
    "                                node_freq_mult=False,\n",
    "                                hyperparams=hyperparams,\n",
    "                                min_history_timesteps=7,\n",
    "                                min_future_timesteps=12,\n",
    "                                return_robot=True)\n",
    "train_data_loader = dict()\n",
    "for node_type_data_set in train_dataset:\n",
    "    node_type_dataloader = utils.data.DataLoader(node_type_data_set,\n",
    "                                                    collate_fn=collate,\n",
    "                                                    pin_memory = True,\n",
    "                                                    batch_size=args.batch_size,\n",
    "                                                    shuffle=args.shuffle,\n",
    "                                                    num_workers=0,\n",
    "                                                    drop_last=True)\n",
    "    train_data_loader[node_type_data_set.node_type] = node_type_dataloader\n",
    "\n",
    "eval_scenes = []\n",
    "with open(eval_data_path, 'rb') as f:\n",
    "    eval_env = dill.load(f, encoding='latin1')\n",
    "eval_scenes = eval_env.scenes\n",
    "eval_dataset = EnvironmentDataset(eval_env,\n",
    "                                hyperparams['state'],\n",
    "                                hyperparams['pred_state'],\n",
    "                                scene_freq_mult=False,\n",
    "                                node_freq_mult=False,\n",
    "                                hyperparams=hyperparams,\n",
    "                                min_history_timesteps=7,\n",
    "                                min_future_timesteps=12,\n",
    "                                return_robot=True)\n",
    "eval_data_loader = dict()\n",
    "for node_type_data_set in eval_dataset:\n",
    "    node_type_dataloader = utils.data.DataLoader(node_type_data_set,\n",
    "                                                    collate_fn=collate,\n",
    "                                                    pin_memory=True,\n",
    "                                                    batch_size=args.eval_batch_size,\n",
    "                                                    shuffle=args.shuffle,\n",
    "                                                    num_workers=0,\n",
    "                                                    drop_last=True)\n",
    "    eval_data_loader[node_type_data_set.node_type] = node_type_dataloader\n",
    "\n",
    "logger.info(\"----dataset end----\")\n",
    "\n",
    "\n",
    "logger.info(\"----model begin----\")\n",
    "\n",
    "\n",
    "class ConcatSquashLinear(Module):\n",
    "    def __init__(self, dim_in, dim_out, dim_ctx):\n",
    "        super(ConcatSquashLinear, self).__init__()\n",
    "        self._layer = Linear(dim_in, dim_out)\n",
    "        self._hyper_bias = Linear(dim_ctx, dim_out, bias=False)\n",
    "        self._hyper_gate = Linear(dim_ctx, dim_out)\n",
    "\n",
    "    def forward(self, ctx, x):\n",
    "        gate = torch.sigmoid(self._hyper_gate(ctx))\n",
    "        bias = self._hyper_bias(ctx)\n",
    "        ret = self._layer(x) * gate + bias\n",
    "\n",
    "        return ret\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[: x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_size=(1024, 512), activation='relu', discrim=False, dropout=-1):\n",
    "        super(MLP, self).__init__()\n",
    "        dims = []\n",
    "        dims.append(input_dim)\n",
    "        dims.extend(hidden_size)\n",
    "        dims.append(output_dim)\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(len(dims)-1):\n",
    "            self.layers.append(nn.Linear(dims[i], dims[i+1]))\n",
    "\n",
    "        if activation == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation = nn.Sigmoid()\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid() if discrim else None\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.layers)):\n",
    "            x = self.layers[i](x)\n",
    "            if i != len(self.layers)-1:\n",
    "                x = self.activation(x)\n",
    "                if self.dropout != -1:\n",
    "                    x = nn.Dropout(min(0.1, self.dropout/3) if i == 1 else self.dropout)(x)\n",
    "            elif self.sigmoid:\n",
    "                x = self.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "class AdditiveAttention(nn.Module):\n",
    "    # Implementing the attention module of Bahdanau et al. 2015 where\n",
    "    # score(h_j, s_(i-1)) = v . tanh(W_1 h_j + W_2 s_(i-1))\n",
    "    def __init__(self, encoder_hidden_state_dim, decoder_hidden_state_dim, internal_dim=None):\n",
    "        super(AdditiveAttention, self).__init__()\n",
    "\n",
    "        if internal_dim is None:\n",
    "            internal_dim = int((encoder_hidden_state_dim + decoder_hidden_state_dim) / 2)\n",
    "\n",
    "        self.w1 = nn.Linear(encoder_hidden_state_dim, internal_dim, bias=False)\n",
    "        self.w2 = nn.Linear(decoder_hidden_state_dim, internal_dim, bias=False)\n",
    "        self.v = nn.Linear(internal_dim, 1, bias=False)\n",
    "\n",
    "    def score(self, encoder_state, decoder_state):\n",
    "        # encoder_state is of shape (batch, enc_dim)\n",
    "        # decoder_state is of shape (batch, dec_dim)\n",
    "        # return value should be of shape (batch, 1)\n",
    "        return self.v(torch.tanh(self.w1(encoder_state) + self.w2(decoder_state)))\n",
    "\n",
    "    def forward(self, encoder_states, decoder_state):\n",
    "        # encoder_states is of shape (batch, num_enc_states, enc_dim)\n",
    "        # decoder_state is of shape (batch, dec_dim)\n",
    "        score_vec = torch.cat([self.score(encoder_states[:, i], decoder_state) for i in range(encoder_states.shape[1])],\n",
    "                              dim=1)\n",
    "        # score_vec is of shape (batch, num_enc_states)\n",
    "\n",
    "        attention_probs = torch.unsqueeze(F.softmax(score_vec, dim=1), dim=2)\n",
    "        # attention_probs is of shape (batch, num_enc_states, 1)\n",
    "\n",
    "        final_context_vec = torch.sum(attention_probs * encoder_states, dim=1)\n",
    "        # final_context_vec is of shape (batch, enc_dim)\n",
    "\n",
    "        return final_context_vec, attention_probs\n",
    "    \n",
    "class Model_Dim_Up(Module):\n",
    "    def __init__(self):\n",
    "        super(Model_Dim_Up, self).__init__()\n",
    "        # bs, 1, 2 --> bs, 1, 5\n",
    "\n",
    "        # self.cnn_up = MLP(input_dim = 2, output_dim = 5, hidden_size=[16,64])\n",
    "        self.cnn_up = nn.Conv1d(2, 5, 1, padding=0)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=x.permute(0,2,1)\n",
    "        x = self.cnn_up(x) \n",
    "        x = x.permute(0,2,1)\n",
    "\n",
    "        return x\n",
    "    \n",
    "# model1=Model_Dim_Up()\n",
    "# data1=torch.randn(64,1,2)\n",
    "# out=model1(data1)\n",
    "# print(out.size())\n",
    "# # torch.Size([64, 1, 5])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Model_Encoder_His(Module):\n",
    "    def __init__(self):\n",
    "        super(Model_Encoder_His, self).__init__()\n",
    "        # bs, time, 6 --> bs, 128\n",
    "\n",
    "        self.encoder_his =nn.LSTM(input_size=6, hidden_size=128, batch_first=True).cuda()\n",
    "        self.dropout = nn.Dropout(p=0.25)\n",
    "\n",
    "\n",
    "    def forward(self, node_history_st):\n",
    "    \n",
    "        his_feat, _ = self.encoder_his(node_history_st)  \n",
    "        his_feat = self.dropout(his_feat)\n",
    "        his_feat = his_feat[:,-1,:]\n",
    "\n",
    "        return his_feat\n",
    "\n",
    "\n",
    "class Model_Encoder_Nei(Module):\n",
    "    def __init__(self):\n",
    "        super(Model_Encoder_Nei, self).__init__()\n",
    "        # bs, time, 6 --> bs, 128\n",
    "\n",
    "        self.encoder_nei =nn.LSTM(input_size=12, hidden_size=128, batch_first=True).cuda()\n",
    "        self.encoder_combine=AdditiveAttention(encoder_hidden_state_dim=128, decoder_hidden_state_dim=128).cuda()\n",
    "        self.dropout = nn.Dropout(p=0.25)\n",
    "\n",
    "        self.state = hyperparams['state']\n",
    "\n",
    "    def forward(self, node_history_st, neighbors, neighbors_edge_value, edge_type, his_feat):\n",
    "    \n",
    "\n",
    "        edge_states_list = list()  \n",
    "        for i, neighbor_states in enumerate(neighbors):  \n",
    "            if len(neighbor_states) == 0:  # There are no neighbors for edge type # TODO necessary?\n",
    "                neighbor_state_length = int(\n",
    "                    np.sum([len(entity_dims) for entity_dims in self.state[edge_type[1]].values()])\n",
    "                ) # 6\n",
    "                edge_states_list.append(torch.zeros(1, 8, neighbor_state_length).cuda())\n",
    "            else:\n",
    "                edge_states_list.append(torch.stack(neighbor_states, dim=0).cuda())\n",
    "        \n",
    "        op_applied_edge_states_list = list()\n",
    "        for neighbors_state in edge_states_list:\n",
    "            op_applied_edge_states_list.append(torch.sum(neighbors_state, dim=0)) #  list of [max_ht, state_dim] torch.Size([8, 6])\n",
    "        combined_neighbors = torch.stack(op_applied_edge_states_list, dim=0) # torch.Size([256, 8, 6])\n",
    "\n",
    "        op_applied_edge_mask_list = list()\n",
    "        for edge_value in neighbors_edge_value:\n",
    "            op_applied_edge_mask_list.append(torch.clamp(torch.sum(edge_value.cuda(), dim=0, keepdim=True), max=1.))\n",
    "        combined_edge_masks = torch.stack(op_applied_edge_mask_list, dim=0) # torch.Size([256, 1])\n",
    "\n",
    "        joint_history = torch.cat([combined_neighbors, node_history_st], dim=-1)\n",
    "\n",
    "        nei_feat, _ = self.encoder_nei(joint_history) \n",
    "        nei_feat = self.dropout(nei_feat)\n",
    "        nei_feat = nei_feat[:,-1,:]\n",
    "\n",
    "        nei_feat = nei_feat * combined_edge_masks\n",
    "\n",
    "        nei_feats = torch.stack([nei_feat], dim=1)\n",
    "\n",
    "        combined_feat, _ = self.encoder_combine(nei_feats, his_feat)\n",
    "        combined_feat = self.dropout(combined_feat)\n",
    "\n",
    "\n",
    "        return combined_feat\n",
    "\n",
    "# model1=Model_Encoder_His().cuda()\n",
    "# model2=Model_Encoder_Nei().cuda()\n",
    "\n",
    "# for node_type, data_loader in train_data_loader.items():\n",
    "#     break\n",
    "# for batch in data_loader:\n",
    "#     break\n",
    "# edge_type=train_env.get_edge_types()[0]\n",
    "# (first_history_index,\n",
    "#     x_t, y_t, x_st_t, y_st_t, # y_t torch.Size([256, 12, 2])\n",
    "#     neighbors_data_st,\n",
    "#     neighbors_edge_value,\n",
    "#     robot_traj_st_t,\n",
    "#     map) = batch\n",
    "\n",
    "# his_f=model1(x_st_t.cuda())\n",
    "# print(his_f.size())\n",
    "# # torch.Size([256, 128])\n",
    "# nei_f=model2(x_st_t.cuda(),restore(neighbors_data_st)[edge_type],restore(neighbors_edge_value)[edge_type],edge_type, his_f)\n",
    "# print(nei_f.size())\n",
    "# # torch.Size([256, 128])\n",
    "\n",
    "\n",
    "\n",
    "class Model_backbone(Module):\n",
    "    def __init__(self):\n",
    "        super(Model_backbone, self).__init__()\n",
    "\n",
    "        context_dim=256\n",
    "        dim=5\n",
    "        self.pos_emb = PositionalEncoding(d_model=2*context_dim, dropout=0.1, max_len=24)\n",
    "        self.concat1 = ConcatSquashLinear(dim,2*context_dim,context_dim+3)\n",
    "        self.layer = nn.TransformerEncoderLayer(d_model=2*context_dim, nhead=4, dim_feedforward=4*context_dim)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.layer, num_layers=3)\n",
    "        self.concat3 = ConcatSquashLinear(2*context_dim,context_dim,context_dim+3)\n",
    "        self.concat4 = ConcatSquashLinear(context_dim,context_dim//2,context_dim+3)\n",
    "        self.linear = ConcatSquashLinear(context_dim//2, dim, context_dim+3)\n",
    "        #self.linear = nn.Linear(128,2)\n",
    "\n",
    "\n",
    "    def forward(self, endpoint_feat, beta, guide):\n",
    "        # bs,1,5 bs,8,256\n",
    "\n",
    "        batch_size = endpoint_feat.size(0)\n",
    "        beta = beta.view(batch_size, 1, 1)          # (B, 1, 1)\n",
    "        guide = guide.view(batch_size, 1, -1)   # (B, 1, F)\n",
    "\n",
    "        time_emb = torch.cat([beta, torch.sin(beta), torch.cos(beta)], dim=-1)  # (B, 1, 3)\n",
    "        ctx_emb = torch.cat([time_emb, guide], dim=-1)    # (B, 1, F+3)\n",
    "        endpoint_feat = self.concat1(ctx_emb,endpoint_feat)\n",
    "        final_emb = endpoint_feat.permute(1,0,2)\n",
    "        final_emb = self.pos_emb(final_emb)\n",
    "\n",
    "\n",
    "        trans = self.transformer_encoder(final_emb).permute(1,0,2)\n",
    "        trans = self.concat3(ctx_emb, trans)\n",
    "        trans = self.concat4(ctx_emb, trans)\n",
    "        return self.linear(ctx_emb, trans)\n",
    "\n",
    "# model1=Model_backbone()\n",
    "# endpoint_feat=torch.randn(64,1,5)\n",
    "# beta=torch.randn(64)\n",
    "# guide=torch.randn(64,256)\n",
    "# model1(endpoint_feat,beta,guide).size()\n",
    "# # torch.Size([64, 1, 5])\n",
    "\n",
    "class Model_his_to_end(Module):\n",
    "    def __init__(self):\n",
    "        super(Model_his_to_end, self).__init__()\n",
    "        # bs, 128 --> bs, 5\n",
    "\n",
    "        self.encoder_his = MLP(input_dim = 128, output_dim = 5, hidden_size=[32, 8])\n",
    "\n",
    "    def forward(self, his_feat):\n",
    "        end_list=[]\n",
    "        end_feat = self.encoder_his(his_feat) # bs,5\n",
    "\n",
    "        sx = torch.exp(end_feat[:, 2])  # sx\n",
    "        sy = torch.exp(end_feat[ :, 3])  # sy\n",
    "        corr = torch.tanh(end_feat[ :, 4])  # corr\n",
    "\n",
    "        cov = torch.zeros(end_feat.shape[0], 2, 2).to('cuda')\n",
    "        cov[:, 0, 0] = sx * sx\n",
    "        cov[:, 0, 1] = corr * sx * sy\n",
    "        cov[:, 1, 0] = corr * sx * sy\n",
    "        cov[:, 1, 1] = sy * sy\n",
    "        mean = end_feat[:, 0:2]\n",
    "        mvn = torch.distributions.multivariate_normal.MultivariateNormal(mean, cov)\n",
    "\n",
    "        for i in range(args.end_list):\n",
    "\n",
    "            end_list.append(mvn.sample())\n",
    "\n",
    "        return end_list\n",
    "    #  10* torch.Size([64,2])\n",
    "# model1=Model_his_to_end()\n",
    "# data1=torch.randn(64,128)\n",
    "# model1(data1)[0].size()\n",
    "\n",
    "\n",
    "class Model_all_to_pred(Module):\n",
    "    def __init__(self):\n",
    "        super(Model_all_to_pred, self).__init__()\n",
    "\n",
    "        context_dim=256\n",
    "\n",
    "        dim=2\n",
    "\n",
    "        self.his_pred = MLP(input_dim = 8, output_dim = 11, hidden_size=[32,128])\n",
    "\n",
    "        self.encoder_end = MLP(input_dim = 2, output_dim = 128, hidden_size=[8,32])\n",
    "\n",
    "        self.pos_emb = PositionalEncoding(d_model=2*context_dim, dropout=0.1, max_len=24)\n",
    "        self.concat1 = ConcatSquashLinear(dim,2*context_dim,context_dim)\n",
    "        self.layer = nn.TransformerEncoderLayer(d_model=2*context_dim, nhead=4, dim_feedforward=4*context_dim)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.layer, num_layers=3)\n",
    "        self.concat3 = ConcatSquashLinear(2*context_dim,context_dim,context_dim)\n",
    "        self.concat4 = ConcatSquashLinear(context_dim,context_dim//2,context_dim)\n",
    "        self.linear = ConcatSquashLinear(context_dim//2, 5, context_dim)\n",
    "\n",
    "    def forward(self, his, end, his_feat, nei_feat):\n",
    "        bs=end.size()[0]\n",
    "\n",
    "        his=his[:,:,2:4]\n",
    "        his_p=his.permute(0,2,1)\n",
    "        his_pred=self.his_pred(his_p)\n",
    "        his_pred=his_pred.permute(0,2,1)\n",
    "        his_all=torch.concat((his_pred,end.view(bs, 1, -1)),dim=1) #bs,12,2\n",
    "        ctx_emb=torch.concat((his_feat,nei_feat),dim=1).view(bs, 1, -1)\n",
    "\n",
    "        pred_feat = self.concat1(ctx_emb,his_all)\n",
    "        final_emb = pred_feat.permute(1,0,2)\n",
    "        final_emb = self.pos_emb(final_emb)\n",
    "\n",
    "        trans = self.transformer_encoder(final_emb).permute(1,0,2)\n",
    "        trans = self.concat3(ctx_emb, trans)\n",
    "        trans = self.concat4(ctx_emb, trans)\n",
    "        return self.linear(ctx_emb, trans)\n",
    "\n",
    "# model1=Model_all_to_pred()\n",
    "# his=torch.randn(64,8,2)\n",
    "# end=torch.randn(64,2)\n",
    "# his_feat=torch.randn(64,128)\n",
    "# nei_feat=torch.randn(64,128)\n",
    "# model1(his,end,his_feat,nei_feat).size()\n",
    "# # torch.Size([64, 12, 5])\n",
    "\n",
    "\n",
    "class Model_diffusion(Module):\n",
    "    def __init__(self):\n",
    "        super(Model_diffusion, self).__init__()\n",
    "\n",
    "        self.backbone=Model_backbone().cuda()    \n",
    "        self.var_sched = VarianceSchedule(\n",
    "                num_steps=args.num_steps,\n",
    "                beta_T=5e-2,\n",
    "                mode='linear',\n",
    "            ).cuda()    \n",
    "\n",
    "\n",
    "    def get_loss(self, model_up, endpoint, his,nei,mask,edge_type,his_en,nei_en):\n",
    "\n",
    "        endpoint_oir=torch.clone(endpoint) #bs,1,2\n",
    "        endpoint=model_up(endpoint)#bs,1,5\n",
    "\n",
    "\n",
    "        sx = torch.exp(endpoint[:, :, 2])  # sx\n",
    "        sy = torch.exp(endpoint[:, :, 3])  # sy\n",
    "        corr = torch.tanh(endpoint[:, :, 4])  # corr\n",
    "        cov = torch.zeros(endpoint.shape[0], endpoint.shape[1], 2, 2).to('cuda')\n",
    "        cov[:, :, 0, 0] = sx * sx\n",
    "        cov[:, :, 0, 1] = corr * sx * sy\n",
    "        cov[:, :, 1, 0] = corr * sx * sy\n",
    "        cov[:, :, 1, 1] = sy * sy\n",
    "        mean = endpoint[:, :, 0:2] # bs,1,2\n",
    "        mvn = torch.distributions.multivariate_normal.MultivariateNormal(mean, cov)\n",
    "        loss_gau = - mvn.log_prob(endpoint_oir).sum()\n",
    "        loss_mean = F.mse_loss(mean.contiguous().view(-1, 2), endpoint_oir.contiguous().view(-1, 2), reduction='mean')\n",
    "\n",
    "        his_feat=his_en(his)\n",
    "        nei_feat=nei_en(his, nei, mask, edge_type, his_feat)\n",
    "        guide= torch.concat((his_feat,nei_feat),dim=1)\n",
    "\n",
    "        batch_size, _, point_dim = endpoint.size() #$ bs,1,5\n",
    "\n",
    "        t = self.var_sched.uniform_sample_t(batch_size) # 256 t \n",
    "\n",
    "        alpha_bar = self.var_sched.alpha_bars[t]\n",
    "        beta = self.var_sched.betas[t].cuda()\n",
    "\n",
    "        c0 = torch.sqrt(alpha_bar).view(-1, 1, 1).cuda()       # (B, 1, 1)\n",
    "        c1 = torch.sqrt(1 - alpha_bar).view(-1, 1, 1).cuda()   # (B, 1, 1)\n",
    "\n",
    "        e_rand = torch.randn_like(endpoint).cuda()  # (B, N, d) torch.Size([256, 12, 2])\n",
    "\n",
    "        e_theta = self.backbone(c0 * endpoint + c1 * e_rand, beta, guide) # torch.Size([256, 12, 2])\n",
    "\n",
    "        loss_diffusion = F.mse_loss(e_theta.contiguous().view(-1, point_dim), e_rand.contiguous().view(-1, point_dim), reduction='mean')\n",
    "        \n",
    "        return loss_diffusion ,loss_gau ,loss_mean, endpoint, his_feat,nei_feat,guide\n",
    "\n",
    "\n",
    "\n",
    "    def sample(self, model_up, his,nei,mask,edge_type,his_en,nei_en):\n",
    "        gau_up=model_up\n",
    "\n",
    "        traj_list = []\n",
    "        point_dim=args.point_dim\n",
    "        num_points=args.pred_length\n",
    "        self.alphas_cumprod = self.var_sched.alpha_bars\n",
    "\n",
    "        his_feat=his_en(his)\n",
    "        nei_feat=nei_en(his, nei, mask, edge_type, his_feat)\n",
    "        guide= torch.concat((his_feat,nei_feat),dim=1)\n",
    "\n",
    "        for diff_sample_num in range(args.diffusion_sample_num):\n",
    "        \n",
    "            batch_size = guide.size(0)\n",
    "\n",
    "            ddim_timesteps=args.num_ddim\n",
    "            ddim_eta=args.ddim_eta\n",
    "            clip_denoised=args.clip_denoised\n",
    "\n",
    "            c = self.var_sched.num_steps // ddim_timesteps\n",
    "            ddim_timestep_seq = np.asarray(list(range(0, self.var_sched.num_steps, c)))\n",
    "            # add one to get the final alpha values right (the ones from first scale to data during sampling)\n",
    "            ddim_timestep_seq = ddim_timestep_seq + 1\n",
    "            # previous sequence\n",
    "            ddim_timestep_prev_seq = np.append(np.array([0]), ddim_timestep_seq[:-1])\n",
    "\n",
    "\n",
    "\n",
    "            sample_img = torch.randn([batch_size, num_points, point_dim]).to(guide.device)\n",
    "            sample_img=gau_up(sample_img)\n",
    "\n",
    "\n",
    "            ddim_timesteps_test = ddim_timesteps\n",
    "            # ddim_timesteps_test = self.config.ddim_timesteps_test\n",
    "            for i in reversed(range(0, ddim_timesteps_test)) :\n",
    "                t = torch.full((batch_size,), ddim_timestep_seq[i], device=guide.device, dtype=torch.long)\n",
    "                prev_t = torch.full((batch_size,), ddim_timestep_prev_seq[i], device=guide.device, dtype=torch.long)\n",
    "                \n",
    "                # 1. get current and previous alpha_cumprod\n",
    "                \n",
    "                alpha_cumprod_t = extract(self.alphas_cumprod, t, sample_img.shape)\n",
    "                alpha_cumprod_t_prev = extract(self.alphas_cumprod, prev_t, sample_img.shape)\n",
    "        \n",
    "                # 2. predict noise using model\n",
    "                beta = self.var_sched.betas[[t[0].item()]*batch_size]\n",
    "                pred_noise = self.backbone(sample_img, beta, guide)\n",
    "                \n",
    "                # 3. get the predicted x_0\n",
    "                pred_x0 = (sample_img - torch.sqrt((1. - alpha_cumprod_t)) * pred_noise) / torch.sqrt(alpha_cumprod_t)\n",
    "                if clip_denoised:\n",
    "                    pred_x0 = torch.clamp(pred_x0, min=-1., max=1.)\n",
    "                \n",
    "                # 4. compute variance: \"sigma_t(η)\" -> see formula (16)\n",
    "                # σ_t = sqrt((1 − α_t−1)/(1 − α_t)) * sqrt(1 − α_t/α_t−1)\n",
    "                sigmas_t = ddim_eta * torch.sqrt(\n",
    "                    (1 - alpha_cumprod_t_prev) / (1 - alpha_cumprod_t) * (1 - alpha_cumprod_t / alpha_cumprod_t_prev))\n",
    "                \n",
    "                # 5. compute \"direction pointing to x_t\" of formula (12)\n",
    "                pred_dir_xt = torch.sqrt(1 - alpha_cumprod_t_prev - sigmas_t**2) * pred_noise\n",
    "                \n",
    "                # 6. compute x_{t-1} of formula (12)\n",
    "                x_prev = torch.sqrt(alpha_cumprod_t_prev) * pred_x0 + pred_dir_xt + sigmas_t * torch.randn_like(sample_img)\n",
    "\n",
    "                sample_img = x_prev.detach()\n",
    "            traj_list.append(sample_img)\n",
    "\n",
    "        return traj_list,his_feat,nei_feat,guide\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class OUR():\n",
    "    def __init__(self):\n",
    "        super(OUR, self).__init__()\n",
    "        self.gau_up=Model_Dim_Up().cuda() \n",
    "        self.his_en=Model_Encoder_His().cuda() \n",
    "        self.nei_en=Model_Encoder_Nei().cuda() \n",
    "        self.model_diffuion=Model_diffusion().cuda() \n",
    "\n",
    "        self.model_end=Model_his_to_end().cuda()\n",
    "        self.model_pred=Model_all_to_pred().cuda()\n",
    "\n",
    "        self.train_dataset=train_dataset\n",
    "        self.hyperparams = hyperparams\n",
    "        \n",
    "\n",
    "        self.optimizer_right = optim.Adam([{'params': self.gau_up.parameters()},\n",
    "                                     {'params': self.model_diffuion.parameters()},\n",
    "                                     {'params': self.his_en.parameters()},\n",
    "                                     {'params': self.nei_en.parameters()},\n",
    "                                    ],\n",
    "                                    lr=args.lr)\n",
    "        self.scheduler_right = optim.lr_scheduler.ExponentialLR(self.optimizer_right,gamma=args.gamma)\n",
    "\n",
    "        self.optimizer_left = optim.Adam([{'params': self.model_end.parameters()},\n",
    "                                     {'params': self.model_pred.parameters()}\n",
    "                                    ],\n",
    "                                    lr=args.lr2)\n",
    "        self.scheduler_left = optim.lr_scheduler.ExponentialLR(self.optimizer_left,gamma=args.gamma)\n",
    "        \n",
    "    def train(self):\n",
    "        self.gau_up.train()\n",
    "        self.his_en.train()\n",
    "        self.nei_en.train()\n",
    "        self.model_diffuion.train()\n",
    "\n",
    "        self.model_end.train()\n",
    "        self.model_pred.train()\n",
    "\n",
    "        ade_final=9999\n",
    "        fde_final=9999\n",
    "        ade = 999\n",
    "        fde = 999\n",
    "        ade_epoch=0\n",
    "        fde_epoch=0\n",
    "        ftimesum=0.\n",
    "        btimesum=0.\n",
    "        sample_count=0\n",
    "\n",
    "        for epoch in range(1, args.epochs + 1):\n",
    "            self.gau_up.train()\n",
    "            self.his_en.train()\n",
    "            self.nei_en.train()\n",
    "            self.model_diffuion.train()\n",
    "\n",
    "            self.model_end.train()\n",
    "            self.model_pred.train()\n",
    "\n",
    "            start_time_f = time.time()\n",
    "            self.train_dataset.augment = args.augment\n",
    "            train_count=0\n",
    "            select_fde_sum=0.0\n",
    "            for node_type, data_loader in train_data_loader.items():\n",
    "                pbar = tqdm(data_loader, ncols=80)\n",
    "                right_loss=0.0\n",
    "                right_loss_diff=0.0\n",
    "                right_loss_gau=0.0\n",
    "                right_loss_mean=0.0\n",
    "\n",
    "                left_loss=0.0\n",
    "                left_loss_gau=0.0\n",
    "                left_loss_mean=0.0\n",
    "                left_loss_select = 0.0\n",
    "\n",
    "                count=0\n",
    "                for batch in pbar:\n",
    "                    edge_type=train_env.get_edge_types()[0]\n",
    "                    (first_history_index,\n",
    "                        x_t, y_t, x_st_t, y_st_t, # y_t torch.Size([256, 12, 2])\n",
    "                        neighbors_data_st,\n",
    "                        neighbors_edge_value,\n",
    "                        robot_traj_st_t,\n",
    "                        map) = batch\n",
    "\n",
    "                    self.his=x_st_t.cuda()\n",
    "                    self.gt=y_st_t.cuda()\n",
    "                    self.end=self.gt[:,11:12,:]\n",
    "                    self.nei=restore(neighbors_data_st)[edge_type]\n",
    "                    self.nei_mask=restore(neighbors_edge_value)[edge_type]\n",
    "\n",
    "                    self.optimizer_left.zero_grad()\n",
    "\n",
    "                    # self.optimizer_right.zero_grad()\n",
    "                    right_loss1,right_loss2,right_loss3, end_feat, his_feat,nei_feat,guide = self.model_diffuion.get_loss(self.gau_up, self.end, self.his,self.nei, self.nei_mask,edge_type,self.his_en,self.nei_en)\n",
    "                    train_loss_right = right_loss1*args.loss_diffusion_rate+right_loss2/args.loss_gau_rate+right_loss3*args.loss_mean_rate\n",
    "                    pbar.set_description(f\"Epoch {epoch}, {node_type} Right-MSE: {train_loss_right.item():.2f}\")\n",
    "                    count = count+1\n",
    "                    right_loss = right_loss + train_loss_right.item()\n",
    "                    right_loss_diff = right_loss_diff + right_loss1.item()\n",
    "                    right_loss_gau = right_loss_gau + right_loss2.item()\n",
    "                    right_loss_mean = right_loss_mean + right_loss3.item()\n",
    "                    # train_loss_right.backward(retain_graph=True)\n",
    "                    # self.optimizer_right.step()\n",
    "\n",
    "                \n",
    "                    # self.optimizer_left.zero_grad()\n",
    "\n",
    "                    end_list=self.model_end(his_feat)\n",
    "                    selected_end, probs=find_end(end_feat, end_list)\n",
    "                    # left_loss3 = 1/probs\n",
    "                    select_fde=get_select_fde(selected_end,self.end)\n",
    "                    select_fde_sum=select_fde_sum+select_fde\n",
    "                    \n",
    "                    pred=self.model_pred(self.his, selected_end, his_feat, nei_feat)\n",
    "                    left_loss1, left_loss2 = get_pred_loss(pred, selected_end, self.gt)\n",
    "                    # train_loss_left= left_loss1/args.loss_gau_rate+left_loss2*args.loss_mean_rate\n",
    "                    train_loss_left=left_loss1/args.loss_gau_rate+train_loss_right\n",
    "                    pbar.set_description(f\"Epoch {epoch}, {node_type} Left-MSE: {train_loss_left.item():.2f}\")\n",
    "                    left_loss = left_loss + train_loss_left.item()\n",
    "                    left_loss_gau = left_loss_gau + left_loss1.item()\n",
    "                    left_loss_mean = left_loss_mean + left_loss2.item()\n",
    "                    # left_loss_select = left_loss_select + left_loss3.item()\n",
    "                    train_count=train_count+1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    train_loss_left.backward()\n",
    "                    self.optimizer_left.step()\n",
    "\n",
    "                    break\n",
    "\n",
    "\n",
    "            select_fde=select_fde_sum/train_count\n",
    "\n",
    "            if args.dataset == \"eth\":\n",
    "                select_fde=select_fde/0.6\n",
    "            elif args.dataset == \"sdd\":\n",
    "                select_fde=select_fde* 50\n",
    "\n",
    "\n",
    "            end_time_f = time.time()\n",
    "            ftime= end_time_f - start_time_f\n",
    "            ftimesum = ftimesum+ftime\n",
    "            logger.info(f\"Epoch {epoch}, {node_type} Right-MSE: {(right_loss/count):.2f}, loss1 MSE: {(right_loss_diff/count):.2f}, loss2 MSE: {(right_loss_gau/count):.2f}, loss3 MSE: {(right_loss_mean/count):.2f}, train_time: {(ftime):.2f}, train_time_avg: {(ftimesum/epoch):.2f}\")\n",
    "            logger.info(f\"Epoch {epoch}, {node_type} Left-MSE: {(left_loss/count):.2f}, loss1 gau: {(left_loss_gau/count):.2f}, loss2 MSE: {(left_loss_mean/count):.2f}, train_time: {(ftime):.2f}, train_time_avg: {(ftimesum/epoch):.2f}\")\n",
    "            logger.info(f\"DE {epoch}, {node_type},select_FDE: {(select_fde.item()):.2f} \")\n",
    "\n",
    "            \n",
    "            # if ((epoch % args.eval_every == 0) and (epoch > 0)) or epoch==1:\n",
    "            with torch.no_grad():\n",
    "                self.train_dataset.augment = False\n",
    "                start_time_b = time.time()\n",
    "                self.gau_up.eval()\n",
    "                self.his_en.eval()\n",
    "                self.nei_en.eval()\n",
    "                self.model_diffuion.eval()\n",
    "\n",
    "                self.model_end.eval()\n",
    "                self.model_pred.eval()\n",
    "\n",
    "                node_type = \"PEDESTRIAN\"\n",
    "\n",
    "                ade_sum=0.0\n",
    "                fde_sum=0.0\n",
    "                test_count=0\n",
    "\n",
    "                for node_type_test, data_loader_test in eval_data_loader.items():\n",
    "                    pbar2 = tqdm(data_loader_test, ncols=80)\n",
    "                    for test_batch in pbar2:\n",
    "                        (first_history_index,\n",
    "                            x_t, y_t, x_st_t, y_st_t, # y_t torch.Size([256, 12, 2])\n",
    "                            neighbors_data_st,\n",
    "                            neighbors_edge_value,\n",
    "                            robot_traj_st_t,\n",
    "                            map) = test_batch\n",
    "\n",
    "                        self.test_his=x_st_t.cuda()\n",
    "                        self.test_gt=y_st_t.cuda()\n",
    "                        self.test_end=self.test_gt[:,11:12,:]\n",
    "                        self.test_nei=restore(neighbors_data_st)[edge_type]\n",
    "                        self.test_nei_mask=restore(neighbors_edge_value)[edge_type]\n",
    "\n",
    "                        traj_pred_list,his_feat,nei_feat,guide = self.model_diffuion.sample(self.gau_up, self.test_his,self.test_nei,self.test_nei_mask,edge_type,self.his_en,self.nei_en) # bs,1,5\n",
    "                        traj_pred=traj_pred_list[0]\n",
    "                        \n",
    "                        end_list=self.model_end(his_feat)\n",
    "                        selected_end, probs=find_end(traj_pred, end_list)\n",
    "                        pred=self.model_pred(self.test_his, selected_end, his_feat, nei_feat)\n",
    "                        ade, fde = get_pred_de(pred, self.test_gt)\n",
    "                        \n",
    "                        ade_sum=ade_sum+ade\n",
    "                        fde_sum=fde_sum+fde\n",
    "                        test_count=test_count+1\n",
    "\n",
    "                        break\n",
    "\n",
    "                ade=ade_sum/test_count\n",
    "                fde=fde_sum/test_count\n",
    "\n",
    "\n",
    "\n",
    "                if args.dataset == \"eth\":\n",
    "                    ade = ade/0.6\n",
    "                    fde = fde/0.6\n",
    "                elif args.dataset == \"sdd\":\n",
    "                    ade = ade * 50\n",
    "                    fde = fde * 50\n",
    "\n",
    "                end_time_b = time.time()\n",
    "                btime= end_time_b - start_time_b\n",
    "                btimesum = btimesum+btime\n",
    "                sample_count=sample_count+1\n",
    "\n",
    "                \n",
    "                logger.info(f\"{args.folder_date} {args.dataset}  {args.exp}  :Best of 20: Epoch {epoch} (Train) ADE: {ade} FDE: {fde}, sample_time: {(btime):.2f}, sample_time_avg: {(btimesum/sample_count):.2f}\")\n",
    "\n",
    "\n",
    "                save_path = output_dir +'/'+ str(epoch)+ '_model.pt'\n",
    "                torch.save({\n",
    "                            'gau_up': self.gau_up.state_dict(),\n",
    "                            'his_en': self.his_en.state_dict(),\n",
    "                            'nei_en': self.nei_en.state_dict(),\n",
    "                            'model_diffuion': self.model_diffuion.state_dict(),       \n",
    "                            'model_end': self.model_end.state_dict(),  \n",
    "                            'model_pred': self.model_pred.state_dict(),\n",
    "                            'optimizer_right': self.optimizer_right.state_dict(),\n",
    "                            'optimizer_left': self.optimizer_left.state_dict(),\n",
    "                            }, save_path)\n",
    "                logger.info(\"Saved model to:\\n{}\".format(save_path))\n",
    "        \n",
    "            \n",
    "\n",
    "\n",
    "            if (ade_final>ade):\n",
    "                ade_final = ade\n",
    "                ade_epoch = epoch\n",
    "            if (fde_final>fde):\n",
    "                fde_final=fde\n",
    "                fde_epoch = epoch\n",
    "            print(f\"######## Best Of 20 (Train): ADE: {ade_epoch} -- {ade_final} FDE: {fde_epoch} -- {fde_final}\")\n",
    "            logger.info(f\"######## Best Of 20 (Train): ADE: {ade_epoch} -- {ade_final} FDE: {fde_epoch} -- {fde_final}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = OUR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read\n",
    "if (args.read):\n",
    "    checkpoint = torch.load(args.model)\n",
    "    agent.gau_up.load_state_dict(checkpoint[\"gau_up\"])\n",
    "    agent.his_en.load_state_dict(checkpoint[\"his_en\"])\n",
    "    agent.nei_en.load_state_dict(checkpoint[\"nei_en\"])\n",
    "    agent.model_diffuion.load_state_dict(checkpoint[\"model_diffuion\"])\n",
    "    agent.model_end.load_state_dict(checkpoint[\"model_end\"])\n",
    "    agent.model_pred.load_state_dict(checkpoint[\"model_pred\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "        ade_final=9999\n",
    "        fde_final=9999\n",
    "        ade_avg = 999\n",
    "        fde_avg = 999\n",
    "        ade_epoch=0\n",
    "        fde_epoch=0\n",
    "        ftimesum=0.\n",
    "        btimesum=0.\n",
    "        sample_count=0\n",
    "\n",
    "        \n",
    "        epoch=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "            with torch.no_grad():\n",
    "                agent.train_dataset.augment = False\n",
    "                start_time_b = time.time()\n",
    "                agent.gau_up.eval()\n",
    "                agent.his_en.eval()\n",
    "                agent.nei_en.eval()\n",
    "                agent.model_diffuion.eval()\n",
    "\n",
    "                agent.model_end.eval()\n",
    "                agent.model_pred.eval()\n",
    "\n",
    "                node_type = \"PEDESTRIAN\"\n",
    "\n",
    "                ade_sum=0.0\n",
    "                fde_sum=0.0\n",
    "                test_count=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "            with torch.no_grad():                \n",
    "                for node_type_test, data_loader_test in eval_data_loader.items():\n",
    "                    for test_batch in data_loader_test:\n",
    "                        (first_history_index,\n",
    "                            x_t, y_t, x_st_t, y_st_t, # y_t torch.Size([256, 12, 2])\n",
    "                            neighbors_data_st,\n",
    "                            neighbors_edge_value,\n",
    "                            robot_traj_st_t,\n",
    "                            map) = test_batch\n",
    "                        break\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "11840\t342.0\t10.64\t4.9\n",
    "11850\t342.0\t9.73\t4.76\n",
    "11860\t342.0\t8.75\t4.6\n",
    "11870\t342.0\t7.84\t4.56\n",
    "11880\t342.0\t6.88\t4.5\n",
    "11890\t342.0\t5.99\t4.48\n",
    "11900\t342.0\t5.08\t4.47\n",
    "11910\t342.0\t4.17\t4.43\n",
    "\n",
    "11920\t342.0\t3.31\t4.36\n",
    "11930\t342.0\t2.48\t4.12\n",
    "11940\t342.0\t1.68\t3.86\n",
    "11950\t342.0\t0.82\t3.51\n",
    "11960\t342.0\t0.03\t3.29\n",
    "11970\t342.0\t-0.92\t2.98\n",
    "11980\t342.0\t-1.7\t2.72\n",
    "11990\t342.0\t-2.29\t2.29\n",
    "12000\t342.0\t-2.96\t1.67\n",
    "12010\t342.0\t-3.47\t1.06\n",
    "12020\t342.0\t-3.96\t0.4\n",
    "12030\t342.0\t-4.35\t-0.27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.2259, -0.2488, -1.3650, -0.2100,  0.0000,  0.0000],\n",
       "        [ 2.6799, -0.3328, -1.3650, -0.2100,  0.0000,  0.0000],\n",
       "        [ 2.0919, -0.4288, -1.4700, -0.2400, -0.2625, -0.0750],\n",
       "        [ 1.5459, -0.4528, -1.3650, -0.0600,  0.2625,  0.4500],\n",
       "        [ 0.9699, -0.4888, -1.4400, -0.0900, -0.1875, -0.0750],\n",
       "        [ 0.4359, -0.5008, -1.3350, -0.0300,  0.2625,  0.1500],\n",
       "        [-0.1101, -0.5068, -1.3650, -0.0150, -0.0750,  0.0375],\n",
       "        [-0.6561, -0.5308, -1.3650, -0.0600, -0.0000, -0.1125]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.round(x_t[0], decimals=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.170034231609604"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "( -0.6561+3.158120538965762)/0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.4299691672736214"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(-0.5308+3.188781500364173)/0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7400342316096036"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(-0.6561+3.158120538965762-1.2900*0.4-1.2450*0.4)/0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.349965768390397"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(-0.6561+3.158120538965762-2.5560*2)/0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-2.5560"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2900, -0.1050],\n",
       "        [-1.2450, -0.3600],\n",
       "        [-1.2000, -0.3900],\n",
       "        [-1.2900, -0.5250],\n",
       "        [-1.1850, -0.3300],\n",
       "        [-1.4250, -0.4650],\n",
       "        [-1.1700, -0.3900],\n",
       "        [-0.8850, -0.6450],\n",
       "        [-1.0050, -0.9300],\n",
       "        [-0.7650, -0.9150],\n",
       "        [-0.7350, -0.9900],\n",
       "        [-0.5850, -1.0050]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_t[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2900, -0.1050],\n",
       "        [-1.2450, -0.3600],\n",
       "        [-1.2000, -0.3900],\n",
       "        [-1.2900, -0.5250],\n",
       "        [-1.1850, -0.3300],\n",
       "        [-1.4250, -0.4650],\n",
       "        [-1.1700, -0.3900],\n",
       "        [-0.8850, -0.6450],\n",
       "        [-1.0050, -0.9300],\n",
       "        [-0.7650, -0.9150],\n",
       "        [-0.7350, -0.9900],\n",
       "        [-0.5850, -1.0050]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_st_t[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6450, -0.0525],\n",
       "        [-0.6225, -0.1800],\n",
       "        [-0.6000, -0.1950],\n",
       "        [-0.6450, -0.2625],\n",
       "        [-0.5925, -0.1650],\n",
       "        [-0.7125, -0.2325],\n",
       "        [-0.5850, -0.1950],\n",
       "        [-0.4425, -0.3225],\n",
       "        [-0.5025, -0.4650],\n",
       "        [-0.3825, -0.4575],\n",
       "        [-0.3675, -0.4950],\n",
       "        [-0.2925, -0.5025]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_st_t[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        edge_type=train_env.get_edge_types()[0]            \n",
    "        agent.test_his=x_st_t.cuda()\n",
    "        agent.test_gt=y_st_t.cuda()\n",
    "        agent.test_end=agent.test_gt[:,11:12,:]\n",
    "        agent.test_nei=restore(neighbors_data_st)[edge_type]\n",
    "        agent.test_nei_mask=restore(neighbors_edge_value)[edge_type]\n",
    "\n",
    "        traj_pred_list,his_feat,nei_feat,guide = agent.model_diffuion.sample(agent.gau_up, agent.test_his,agent.test_nei,agent.test_nei_mask,edge_type,agent.his_en,agent.nei_en) # bs,1,5\n",
    "        traj_pred=traj_pred_list[0]\n",
    "        \n",
    "        end_list=agent.model_end(his_feat)\n",
    "        selected_end, probs=find_end(traj_pred, end_list)\n",
    "        pred=agent.model_pred(agent.test_his, selected_end, his_feat, nei_feat)\n",
    "        ade, fde = get_pred_de(pred, agent.test_gt)\n",
    "        \n",
    "        ade_sum=ade_sum+ade\n",
    "        fde_sum=fde_sum+fde\n",
    "        test_count=test_count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1516, device='cuda:7', grad_fn=<MeanBackward1>),\n",
       " tensor(0.3497, device='cuda:7', grad_fn=<MeanBackward1>))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_pred_de(pred, agent.test_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5810, -0.0715, -2.0028, -1.8185, -0.0054],\n",
       "        [-0.5810, -0.0715, -2.0028, -1.8185, -0.0054],\n",
       "        [-0.5810, -0.0715, -2.0028, -1.8185, -0.0054],\n",
       "        [-0.5810, -0.0715, -2.0028, -1.8185, -0.0054],\n",
       "        [-0.5810, -0.0715, -2.0028, -1.8185, -0.0054],\n",
       "        [-0.5810, -0.0715, -2.0028, -1.8185, -0.0054],\n",
       "        [-0.5810, -0.0715, -2.0028, -1.8185, -0.0054],\n",
       "        [-0.5810, -0.0715, -2.0028, -1.8185, -0.0054],\n",
       "        [-0.5810, -0.0715, -2.0028, -1.8185, -0.0054],\n",
       "        [-0.5810, -0.0715, -2.0028, -1.8185, -0.0054],\n",
       "        [-0.5810, -0.0715, -2.0028, -1.8185, -0.0054],\n",
       "        [-0.5810, -0.0715, -2.0028, -1.8185, -0.0054]], device='cuda:7',\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[0,:,:] #???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "    pred=pred[0:1]\n",
    "    gt=agent.test_gt[0:1]\n",
    "    predlist=len(pred)\n",
    "    kstep_V_pred_ls = []\n",
    "    gt = gt.permute(1,0,2)*0.4\n",
    "    pred = pred.permute(1,0,2)\n",
    "\n",
    "    sx = torch.exp(pred[:, :, 2])  # sx\n",
    "    sy = torch.exp(pred[:, :, 3])  # sy\n",
    "    corr = torch.tanh(pred[:, :, 4])  # corr\n",
    "\n",
    "    cov = torch.zeros(pred.shape[0], pred.shape[1], 2, 2).to('cuda')\n",
    "    cov[:, :, 0, 0] = sx * sx\n",
    "    cov[:, :, 0, 1] = corr * sx * sy\n",
    "    cov[:, :, 1, 0] = corr * sx * sy\n",
    "    cov[:, :, 1, 1] = sy * sy\n",
    "    mean = pred[:, :, 0:2]\n",
    "    mvn = torch.distributions.multivariate_normal.MultivariateNormal(mean, cov)\n",
    "\n",
    "    KSTEPS=args.sample\n",
    "    for i in range(KSTEPS-1):\n",
    "        kstep_V_pred_ls.append(torch.cumsum((mvn.sample()*0.4), dim=0))  # cat [12, num_person, 2]\n",
    "    kstep_V_pred_ls.append(torch.cumsum(mean*0.4, dim=0))\n",
    "    kstep_V_pred_ls = torch.stack(kstep_V_pred_ls, dim=0) # [KSTEPS, 12, num_person, 2]\n",
    "\n",
    "    # kstep_V_pred = np.concatenate([traj for traj in kstep_V_pred_ls], axis=1) # [12, KSTEPS * num_person, 2]\n",
    "\n",
    "    \"\"\"end of sampling\"\"\"\n",
    "\n",
    "    V_y_rel_to_abs =  torch.cumsum((gt), dim=0) # [12, num_person, 2] speed???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5810, -0.0715, -2.0028, -1.8185, -0.0054]],\n",
       "\n",
       "        [[-0.5810, -0.0715, -2.0028, -1.8185, -0.0054]],\n",
       "\n",
       "        [[-0.5810, -0.0715, -2.0028, -1.8185, -0.0054]],\n",
       "\n",
       "        [[-0.5810, -0.0715, -2.0028, -1.8185, -0.0054]],\n",
       "\n",
       "        [[-0.5810, -0.0715, -2.0028, -1.8185, -0.0054]],\n",
       "\n",
       "        [[-0.5810, -0.0715, -2.0028, -1.8185, -0.0054]],\n",
       "\n",
       "        [[-0.5810, -0.0715, -2.0028, -1.8185, -0.0054]],\n",
       "\n",
       "        [[-0.5810, -0.0715, -2.0028, -1.8185, -0.0054]],\n",
       "\n",
       "        [[-0.5810, -0.0715, -2.0028, -1.8185, -0.0054]],\n",
       "\n",
       "        [[-0.5810, -0.0715, -2.0028, -1.8185, -0.0054]],\n",
       "\n",
       "        [[-0.5810, -0.0715, -2.0028, -1.8185, -0.0054]],\n",
       "\n",
       "        [[-0.5810, -0.0715, -2.0028, -1.8185, -0.0054]]], device='cuda:7',\n",
       "       grad_fn=<PermuteBackward0>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2580, -0.0210]],\n",
       "\n",
       "        [[-0.2490, -0.0720]],\n",
       "\n",
       "        [[-0.2400, -0.0780]],\n",
       "\n",
       "        [[-0.2580, -0.1050]],\n",
       "\n",
       "        [[-0.2370, -0.0660]],\n",
       "\n",
       "        [[-0.2850, -0.0930]],\n",
       "\n",
       "        [[-0.2340, -0.0780]],\n",
       "\n",
       "        [[-0.1770, -0.1290]],\n",
       "\n",
       "        [[-0.2010, -0.1860]],\n",
       "\n",
       "        [[-0.1530, -0.1830]],\n",
       "\n",
       "        [[-0.1470, -0.1980]],\n",
       "\n",
       "        [[-0.1170, -0.2010]]], device='cuda:7')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1452, device='cuda:7', grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(torch.min(torch.norm((kstep_V_pred_ls - V_y_rel_to_abs),dim=3),dim=0)[0],dim=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6600, device='cuda:7', grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(torch.min(torch.norm((kstep_V_pred_ls - V_y_rel_to_abs)[:,-1,:,:],dim=2),dim=0)[0],dim=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2580, -0.0210]],\n",
       "\n",
       "        [[-0.5070, -0.0930]],\n",
       "\n",
       "        [[-0.7470, -0.1710]],\n",
       "\n",
       "        [[-1.0050, -0.2760]],\n",
       "\n",
       "        [[-1.2420, -0.3420]],\n",
       "\n",
       "        [[-1.5270, -0.4350]],\n",
       "\n",
       "        [[-1.7610, -0.5130]],\n",
       "\n",
       "        [[-1.9380, -0.6420]],\n",
       "\n",
       "        [[-2.1390, -0.8280]],\n",
       "\n",
       "        [[-2.2920, -1.0110]],\n",
       "\n",
       "        [[-2.4390, -1.2090]],\n",
       "\n",
       "        [[-2.5560, -1.4100]]], device='cuda:7')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V_y_rel_to_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_all_to_pred(Module):\n",
    "    def __init__(self):\n",
    "        super(Model_all_to_pred, self).__init__()\n",
    "\n",
    "        context_dim=256\n",
    "\n",
    "        dim=2\n",
    "\n",
    "        self.his_pred = MLP(input_dim = 8, output_dim = 11, hidden_size=[32,128])\n",
    "\n",
    "        self.encoder_end = MLP(input_dim = 2, output_dim = 128, hidden_size=[8,32])\n",
    "\n",
    "        self.pos_emb = PositionalEncoding(d_model=2*context_dim, dropout=0.1, max_len=24)\n",
    "        self.concat1 = ConcatSquashLinear(dim,2*context_dim,context_dim)\n",
    "        # self.concat1 = MLP(input_dim = dim, output_dim = 2*context_dim, hidden_size=[16,64])\n",
    "        self.layer = nn.TransformerEncoderLayer(d_model=2*context_dim, nhead=4, dim_feedforward=4*context_dim)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.layer, num_layers=3)\n",
    "\n",
    "        # self.concat3 = ConcatSquashLinear(2*context_dim,context_dim,context_dim)\n",
    "        # self.concat4 = ConcatSquashLinear(context_dim,context_dim//2,context_dim)\n",
    "        # self.linear = ConcatSquashLinear(context_dim//2, 2, context_dim)\n",
    "\n",
    "        self.concat3 = MLP(input_dim = 2*context_dim, output_dim = context_dim, hidden_size=[context_dim])\n",
    "        self.concat4 = MLP(input_dim = context_dim, output_dim = context_dim//2, hidden_size=[context_dim//2])\n",
    "        self.linear = MLP(input_dim = 2*context_dim, output_dim = 5, hidden_size=[128,32])\n",
    "\n",
    "    def forward(self, his, end, his_feat, nei_feat):\n",
    "        bs=end.size()[0]\n",
    "\n",
    "        his=his[:,:,2:4]\n",
    "        his_p=his.permute(0,2,1)\n",
    "        his_pred=self.his_pred(his_p)\n",
    "        his_pred=his_pred.permute(0,2,1)\n",
    "        his_all=torch.concat((his_pred,end.view(bs, 1, -1)),dim=1) #bs,12,2\n",
    "        ctx_emb=torch.concat((his_feat,nei_feat),dim=1).view(bs, 1, -1)\n",
    "\n",
    "        pred_feat = self.concat1(ctx_emb,his_all)\n",
    "        # final_emb = pred_feat.permute(1,0,2)\n",
    "        # final_emb = self.pos_emb(final_emb)\n",
    "\n",
    "        # trans = self.transformer_encoder(final_emb).permute(1,0,2)\n",
    "        # trans = self.concat3(trans)\n",
    "        # trans = self.concat4(trans)\n",
    "        # return trans\n",
    "        # return trans\n",
    "        return self.linear(pred_feat)\n",
    "\n",
    "# model1=Model_all_to_pred()\n",
    "# his=torch.randn(64,8,2)\n",
    "# end=torch.randn(64,2)\n",
    "# his_feat=torch.randn(64,128)\n",
    "# nei_feat=torch.randn(64,128)\n",
    "# model1(his,end,his_feat,nei_feat).size()\n",
    "# # torch.Size([64, 12, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model1=Model_all_to_pred()\n",
    "his=torch.randn(64,8,6)\n",
    "end=torch.randn(64,2)\n",
    "his_feat=torch.randn(64,128)\n",
    "nei_feat=torch.randn(64,128)\n",
    "a=model1(his,end,his_feat,nei_feat)\n",
    "# # torch.Size([64, 12, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3950, -0.9252, -0.0793, -1.2504,  0.0315, -0.9322],\n",
       "        [ 1.1261,  1.1289,  1.5443,  1.0387,  0.0215, -0.5146],\n",
       "        [ 1.4539, -1.0840, -1.3549, -0.7697,  0.9378, -0.4706],\n",
       "        [-3.5725,  0.4224, -0.5222,  0.8918, -0.7805, -0.3696],\n",
       "        [ 3.0232, -0.5208,  0.2574,  2.0493, -0.2110,  0.5919],\n",
       "        [-0.6999,  0.4719, -0.1307, -0.4318, -1.3817, -1.1068],\n",
       "        [-1.0658,  1.3009, -0.3472, -0.5175, -0.0593, -0.2929],\n",
       "        [ 0.5523, -1.2400, -0.3570,  0.0041, -0.5019, -0.0358]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "his[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0780, -0.1960,  0.1360, -0.1776,  0.1773],\n",
       "        [ 0.0787, -0.1979,  0.1364, -0.1778,  0.1787],\n",
       "        [ 0.0773, -0.1829,  0.1402, -0.1782,  0.1849],\n",
       "        [ 0.0784, -0.1870,  0.1405, -0.1789,  0.1866],\n",
       "        [ 0.0783, -0.1866,  0.1406, -0.1788,  0.1866],\n",
       "        [ 0.0768, -0.1885,  0.1371, -0.1777,  0.1780],\n",
       "        [ 0.0769, -0.1969,  0.1345, -0.1773,  0.1747],\n",
       "        [ 0.0799, -0.1929,  0.1408, -0.1800,  0.1894],\n",
       "        [ 0.0786, -0.1951,  0.1372, -0.1778,  0.1800],\n",
       "        [ 0.0794, -0.1956,  0.1385, -0.1786,  0.1833],\n",
       "        [ 0.0779, -0.1878,  0.1391, -0.1781,  0.1831],\n",
       "        [ 0.0843, -0.2277,  0.1392, -0.1835,  0.2014]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "        bs=end.size()[0]\n",
    "\n",
    "        his=his[:,:,2:4]\n",
    "        his_p=his.permute(0,2,1)\n",
    "        his_pred=model1.his_pred(his_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1251,  0.1741],\n",
       "        [-0.0548, -0.1249],\n",
       "        [-0.1651, -0.1375],\n",
       "        [ 0.0018, -0.0606],\n",
       "        [ 0.1468,  0.0495],\n",
       "        [-0.0258, -0.0539],\n",
       "        [ 0.1066,  0.1194],\n",
       "        [ 0.0365,  0.0917],\n",
       "        [-0.1035, -0.1073],\n",
       "        [ 0.0666,  0.1327],\n",
       "        [ 0.0672,  0.1494],\n",
       "        [ 1.3601,  0.8838]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "his_pred=his_pred.permute(0,2,1)\n",
    "his_all=torch.concat((his_pred,end.view(bs, 1, -1)),dim=1) #bs,12,2\n",
    "his_all[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx_emb=torch.concat((his_feat,nei_feat),dim=1).view(bs, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 256])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx_emb.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_feat = model1.concat1(his_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0398,  0.0400,  0.0377,  ..., -0.1422, -0.0050,  0.1068],\n",
       "        [ 0.0497,  0.0653,  0.0212,  ..., -0.1625,  0.0135,  0.1307],\n",
       "        [ 0.0387,  0.0697,  0.0100,  ..., -0.1631,  0.0120,  0.1456],\n",
       "        ...,\n",
       "        [ 0.0372,  0.0453,  0.0374,  ..., -0.1442, -0.0013,  0.1137],\n",
       "        [ 0.0365,  0.0439,  0.0378,  ..., -0.1435, -0.0013,  0.1136],\n",
       "        [ 0.1824, -0.0836, -0.0332,  ..., -0.1403, -0.0141,  0.0457]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_feat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "        final_emb = pred_feat.permute(1,0,2)\n",
    "        final_emb = model1.pos_emb(final_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.4184e-02,  1.1556e+00,  4.1879e-02,  ...,  9.5312e-01,\n",
       "         -5.5635e-03,  1.2298e+00],\n",
       "        [ 9.9023e-01,  6.7288e-01,  0.0000e+00,  ...,  9.3058e-01,\n",
       "          1.5135e-02,  1.2564e+00],\n",
       "        [ 1.0533e+00, -3.8495e-01,  1.0516e+00,  ...,  9.2989e-01,\n",
       "          1.3614e-02,  1.2729e+00],\n",
       "        ...,\n",
       "        [ 4.9926e-01, -9.6206e-01,  7.9307e-01,  ...,  9.5084e-01,\n",
       "         -0.0000e+00,  1.2375e+00],\n",
       "        [-5.6386e-01, -0.0000e+00, -2.0242e-01,  ...,  9.5168e-01,\n",
       "         -2.8929e-04,  1.2373e+00],\n",
       "        [-9.0842e-01, -8.7973e-02, -1.0670e+00,  ...,  9.5526e-01,\n",
       "         -0.0000e+00,  1.1619e+00]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_emb[:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = model1.transformer_encoder(final_emb).permute(1,0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2784,  1.4990,  0.9927,  ...,  1.9007, -0.8883,  1.5714],\n",
       "        [ 2.4264,  0.8835,  0.8440,  ...,  3.0090, -0.8012,  1.6297],\n",
       "        [ 2.2010,  0.3238,  1.4335,  ...,  2.7133, -1.0645,  1.2364],\n",
       "        ...,\n",
       "        [ 1.1439, -0.2598,  1.7011,  ...,  2.3961, -1.3575,  1.2180],\n",
       "        [ 1.1328,  0.4029, -0.1322,  ...,  2.6167, -0.9369,  1.3953],\n",
       "        [ 0.5782,  0.4892, -0.1230,  ...,  2.3207, -0.8784,  1.5273]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "        trans = model1.concat3( trans)\n",
    "        trans = model1.concat4( trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0858,  0.0751,  0.0157,  ..., -0.0413, -0.0873, -0.0002],\n",
       "        [ 0.0936,  0.1002,  0.0119,  ..., -0.0483, -0.0762,  0.0441],\n",
       "        [ 0.0791,  0.0812,  0.0423,  ..., -0.0528, -0.0988,  0.0236],\n",
       "        ...,\n",
       "        [ 0.0726,  0.0194,  0.0148,  ..., -0.0224, -0.1032,  0.0198],\n",
       "        [ 0.0919,  0.0418,  0.0049,  ..., -0.0406, -0.1122,  0.0199],\n",
       "        [ 0.1239,  0.0209, -0.0187,  ..., -0.0363, -0.1187,  0.0244]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = model1.linear(trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-9.5046e-03, -5.4681e-03,  9.7994e-02,  9.7661e-02, -1.2506e-01],\n",
       "        [-6.6531e-03, -7.9715e-03,  9.5394e-02,  1.0098e-01, -1.2522e-01],\n",
       "        [-3.3296e-03, -5.6923e-03,  1.0525e-01,  1.0551e-01, -1.2497e-01],\n",
       "        [-1.0113e-02,  8.0182e-05,  1.0798e-01,  1.0272e-01, -1.2235e-01],\n",
       "        [-1.8890e-03, -2.3680e-03,  1.1414e-01,  1.0417e-01, -1.2713e-01],\n",
       "        [-9.7343e-04, -8.8613e-03,  1.1188e-01,  1.0472e-01, -1.2085e-01],\n",
       "        [-8.1108e-03, -6.7176e-03,  1.0682e-01,  1.0358e-01, -1.1909e-01],\n",
       "        [-1.4995e-02, -1.1553e-02,  8.9808e-02,  9.5716e-02, -1.2188e-01],\n",
       "        [-1.9248e-02, -1.6822e-02,  8.7399e-02,  9.5797e-02, -1.1621e-01],\n",
       "        [-5.9077e-03, -9.9574e-03,  1.0178e-01,  1.0054e-01, -1.1024e-01],\n",
       "        [-7.1790e-03, -2.0059e-02,  1.0323e-01,  1.0043e-01, -1.1305e-01],\n",
       "        [-5.7603e-03, -2.1204e-02,  1.0349e-01,  1.0162e-01, -1.0850e-01]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_his = MLP(input_dim = 128, output_dim = 5, hidden_size=[64,32, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 12, 128])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo=torch.randn(64, 12, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans2 = encoder_his( demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0317,  0.1256,  0.3200,  0.0443,  0.0318],\n",
       "        [-0.0086,  0.1691,  0.4045,  0.0475, -0.0402],\n",
       "        [-0.0386,  0.1369,  0.3025,  0.0492,  0.0349],\n",
       "        [-0.0371,  0.1889,  0.3232,  0.0412, -0.0293],\n",
       "        [-0.0443,  0.1911,  0.3645,  0.0346, -0.0564],\n",
       "        [-0.0315,  0.1505,  0.3637,  0.0516, -0.0082],\n",
       "        [-0.0461,  0.1400,  0.3087,  0.0401,  0.0291],\n",
       "        [-0.0138,  0.1774,  0.4084,  0.0547, -0.0468],\n",
       "        [-0.0452,  0.1362,  0.4030,  0.0323, -0.0331],\n",
       "        [-0.0582,  0.1405,  0.3147,  0.0403,  0.0093],\n",
       "        [-0.0404,  0.1541,  0.4007,  0.0465, -0.0376],\n",
       "        [-0.1017,  0.1753,  0.2808,  0.0053, -0.0314]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy.ndimage import gaussian_filter1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show2(data):\n",
    "    obs,gt,ours= data\n",
    "\n",
    "    ax2.plot(obs[:,0],obs[:,1], color='deepskyblue', zorder=100,alpha=0.6,label='Observation')\n",
    "    ax2.plot(gt[:,0],gt[:,1], color='limegreen', zorder=90,alpha=0.6,label='Ground Truth')\n",
    "    ax2.plot(ours[:,0],ours[:,1], color='red', zorder=80,alpha=0.6, label='Ours')\n",
    "\n",
    "\n",
    "    return ax2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_temp=(\n",
    "    agent.test_his[23].reshape(8,6).cpu().detach().numpy(),\n",
    "    V_y_rel_to_abs.reshape(12,2).detach().cpu().numpy(),\n",
    "    kstep_V_pred_ls[0].reshape(12,2).cpu().detach().numpy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbgAAAEoCAYAAAAqrOTwAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAEzUlEQVR4nO3bsU3EQBRF0e8VEgElbIqogh6gDYqjB2dk9EIFJiHGidcrXZ2TTDDJy64mmGXbtgGAmsu9BwDALQgcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEkCB0CSwAGQJHAAJAkcAEn/Bm5Z52VZ5/msMQBwlL0X3PvMvJ2wAwAOtRe4n5m5Lus8njEGAI6yF7ivmXmamdcTtgDAYfYC9/F3Xm89BACO9LBzf5mZz5n5PmELABxm2bbt3hsA4HD+wQGQJHAAJAkcAEkCB0CSwAGQJHAAJP0CuBEPmW1mvE0AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "fig1, ax2 = plt.subplots(constrained_layout=True)\n",
    "\n",
    "plt.xlim((0, 15))\n",
    "plt.ylim((0, 12))\n",
    "plt.axis('off')\n",
    "\n",
    "show2(data_temp)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3166],\n",
       "        [0.5415],\n",
       "        [0.2162],\n",
       "        [0.4989],\n",
       "        [0.2784],\n",
       "        [0.4850],\n",
       "        [0.2669],\n",
       "        [0.4112],\n",
       "        [0.6227],\n",
       "        [0.3702],\n",
       "        [0.3069],\n",
       "        [0.4192],\n",
       "        [0.4742],\n",
       "        [0.3250],\n",
       "        [0.4460],\n",
       "        [0.6479],\n",
       "        [0.3981],\n",
       "        [0.4366],\n",
       "        [0.3034],\n",
       "        [0.3790]], device='cuda:7', grad_fn=<CopyBackwards>)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm((kstep_V_pred_ls - V_y_rel_to_abs)[:,-1,:,:],dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dill\n",
    "import pickle\n",
    "\n",
    "from environment import Environment, Scene, Node, derivative_of\n",
    "\n",
    "desired_max_time = 100\n",
    "pred_indices = [2, 3]\n",
    "state_dim = 6\n",
    "frame_diff = 10\n",
    "desired_frame_diff = 1\n",
    "dt = 0.4\n",
    "\n",
    "standardization = {\n",
    "    'PEDESTRIAN': {\n",
    "        'position': {\n",
    "            'x': {'mean': 0, 'std': 1},\n",
    "            'y': {'mean': 0, 'std': 1}\n",
    "        },\n",
    "        'velocity': {\n",
    "            'x': {'mean': 0, 'std': 2},\n",
    "            'y': {'mean': 0, 'std': 2}\n",
    "        },\n",
    "        'acceleration': {\n",
    "            'x': {'mean': 0, 'std': 1},\n",
    "            'y': {'mean': 0, 'std': 1}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "def maybe_makedirs(path_to_create):\n",
    "    \"\"\"This function will create a directory, unless it exists already,\n",
    "    at which point the function will return.\n",
    "    The exception handling is necessary as it prevents a race condition\n",
    "    from occurring.\n",
    "    Inputs:\n",
    "        path_to_create - A string path to a directory you'd like created.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        os.makedirs(path_to_create)\n",
    "    except OSError:\n",
    "        if not os.path.isdir(path_to_create):\n",
    "            raise\n",
    "\n",
    "def augment_scene(scene, angle):\n",
    "    def rotate_pc(pc, alpha):\n",
    "        M = np.array([[np.cos(alpha), -np.sin(alpha)],\n",
    "                      [np.sin(alpha), np.cos(alpha)]])\n",
    "        return M @ pc\n",
    "\n",
    "    data_columns = pd.MultiIndex.from_product([['position', 'velocity', 'acceleration'], ['x', 'y']])\n",
    "\n",
    "    scene_aug = Scene(timesteps=scene.timesteps, dt=scene.dt, name=scene.name)\n",
    "\n",
    "    alpha = angle * np.pi / 180\n",
    "\n",
    "    for node in scene.nodes:\n",
    "        x = node.data.position.x.copy()\n",
    "        y = node.data.position.y.copy()\n",
    "\n",
    "        x, y = rotate_pc(np.array([x, y]), alpha)\n",
    "\n",
    "        vx = derivative_of(x, scene.dt)\n",
    "        vy = derivative_of(y, scene.dt)\n",
    "        ax = derivative_of(vx, scene.dt)\n",
    "        ay = derivative_of(vy, scene.dt)\n",
    "\n",
    "        data_dict = {('position', 'x'): x,\n",
    "                     ('position', 'y'): y,\n",
    "                     ('velocity', 'x'): vx,\n",
    "                     ('velocity', 'y'): vy,\n",
    "                     ('acceleration', 'x'): ax,\n",
    "                     ('acceleration', 'y'): ay}\n",
    "\n",
    "        node_data = pd.DataFrame(data_dict, columns=data_columns)\n",
    "\n",
    "        node = Node(node_type=node.type, node_id=node.id, data=node_data, first_timestep=node.first_timestep)\n",
    "\n",
    "        scene_aug.nodes.append(node)\n",
    "    return scene_aug\n",
    "\n",
    "\n",
    "def augment(scene):\n",
    "    scene_aug = np.random.choice(scene.augmented)\n",
    "    scene_aug.temporal_scene_graph = scene.temporal_scene_graph\n",
    "    return scene_aug\n",
    "\n",
    "\n",
    "nl = 0\n",
    "l = 0\n",
    "\n",
    "data_folder_name = 'processed_data_noise'\n",
    "\n",
    "maybe_makedirs(data_folder_name)\n",
    "data_columns = pd.MultiIndex.from_product([['position', 'velocity', 'acceleration'], ['x', 'y']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At raw_data/eth/test/biwi_eth.txt\n",
      "Scene: Duration: 464.40000000000003s, Nodes: 360, Map: No.\n",
      "Processed 1.00 scene for data class test\n"
     ]
    }
   ],
   "source": [
    "for desired_source in ['eth']:\n",
    "    for data_class in ['test']:\n",
    "        env = Environment(node_type_list=['PEDESTRIAN'], standardization=standardization)\n",
    "        attention_radius = dict()\n",
    "        attention_radius[(env.NodeType.PEDESTRIAN, env.NodeType.PEDESTRIAN)] = 3.0\n",
    "        env.attention_radius = attention_radius\n",
    "\n",
    "        scenes = []\n",
    "        data_dict_path = os.path.join(data_folder_name, '_'.join([desired_source, data_class]) + '.pkl')\n",
    "\n",
    "        for subdir, dirs, files in os.walk(os.path.join('raw_data', desired_source, data_class)):\n",
    "            for file in files:\n",
    "                if file.endswith('.txt'):\n",
    "                    input_data_dict = dict()\n",
    "                    full_data_path = os.path.join(subdir, file)\n",
    "                    print('At', full_data_path)\n",
    "\n",
    "                    data = pd.read_csv(full_data_path, sep='\\t', index_col=False, header=None)\n",
    "                    data.columns = ['frame_id', 'track_id', 'pos_x', 'pos_y']\n",
    "                    data['frame_id'] = pd.to_numeric(data['frame_id'], downcast='integer')\n",
    "                    data['track_id'] = pd.to_numeric(data['track_id'], downcast='integer')\n",
    "\n",
    "                    data['frame_id'] = data['frame_id'] // 10\n",
    "\n",
    "                    data['frame_id'] -= data['frame_id'].min()\n",
    "\n",
    "                    data['node_type'] = 'PEDESTRIAN'\n",
    "                    data['node_id'] = data['track_id'].astype(str)\n",
    "\n",
    "                    data.sort_values('frame_id', inplace=True)\n",
    "\n",
    "                    # ? lanni 0.6?\n",
    "                    if desired_source == \"eth\" and data_class == \"test\":\n",
    "                        data['pos_x'] = data['pos_x'] * 0.6\n",
    "                        data['pos_y'] = data['pos_y'] * 0.6\n",
    "\n",
    "\n",
    "                    data['pos_x'] = data['pos_x'] - data['pos_x'].mean()\n",
    "                    data['pos_y'] = data['pos_y'] - data['pos_y'].mean()\n",
    "\n",
    "                    max_timesteps = data['frame_id'].max()\n",
    "\n",
    "                    scene = Scene(timesteps=max_timesteps+1, dt=dt, name=desired_source + \"_\" + data_class, aug_func=augment if data_class == 'train' else None)\n",
    "\n",
    "                    for node_id in pd.unique(data['node_id']):\n",
    "\n",
    "                        node_df = data[data['node_id'] == node_id]\n",
    "\n",
    "                        node_values = node_df[['pos_x', 'pos_y']].values\n",
    "\n",
    "                        if node_values.shape[0] < 2: # 节点大于2，但是包含了间断的时刻没有处理\n",
    "                            continue\n",
    "\n",
    "                        new_first_idx = node_df['frame_id'].iloc[0]\n",
    "\n",
    "                        x = node_values[:, 0]\n",
    "                        y = node_values[:, 1]\n",
    "                        vx = derivative_of(x, scene.dt)\n",
    "                        vy = derivative_of(y, scene.dt)\n",
    "                        ax = derivative_of(vx, scene.dt)\n",
    "                        ay = derivative_of(vy, scene.dt)\n",
    "\n",
    "                        data_dict = {('position', 'x'): x,\n",
    "                                     ('position', 'y'): y,\n",
    "                                     ('velocity', 'x'): vx,\n",
    "                                     ('velocity', 'y'): vy,\n",
    "                                     ('acceleration', 'x'): ax,\n",
    "                                     ('acceleration', 'y'): ay}\n",
    "\n",
    "                        node_data = pd.DataFrame(data_dict, columns=data_columns)\n",
    "                        node = Node(node_type=env.NodeType.PEDESTRIAN, node_id=node_id, data=node_data)\n",
    "                        node.first_timestep = new_first_idx\n",
    "\n",
    "                        scene.nodes.append(node)\n",
    "                    if data_class == 'train':\n",
    "                        scene.augmented = list()\n",
    "                        angles = np.arange(0, 360, 15) if data_class == 'train' else [0]\n",
    "                        for angle in angles:\n",
    "                            scene.augmented.append(augment_scene(scene, angle))\n",
    "\n",
    "                    print(scene)\n",
    "                    scenes.append(scene)\n",
    "        print(f'Processed {len(scenes):.2f} scene for data class {data_class}')\n",
    "\n",
    "        env.scenes = scenes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.158120538965762"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['pos_x'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.188781500364173"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['pos_y'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 创建一个包含3个大小为(4, 1, 2)的随机张量的列表\n",
    "tensor_list = [torch.rand(4, 1, 2) for _ in range(3)]\n",
    "\n",
    "# 创建一个目标张量，大小为(4, 1, 2)\n",
    "target_tensor = torch.rand(4, 1, 2)\n",
    "\n",
    "# 计算每个张量与目标张量之间的欧氏距离\n",
    "distances = [torch.norm(target_tensor - t, dim=(1, 2)) for t in tensor_list]\n",
    "\n",
    "# 找到最接近的索引\n",
    "closest_index = torch.argmin(torch.stack(distances), dim=0)\n",
    "\n",
    "# # 提取最接近的张量\n",
    "# closest_tensor = tensor_list[closest_index]\n",
    "\n",
    "# print(\"目标张量：\", target_tensor)\n",
    "# print(\"最接近的张量：\", closest_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8651/696960610.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Extract the closest tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mclosest_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclosest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Target tensor:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a list of 3 tensors, each with size (4, 1, 2)\n",
    "tensor_list = [torch.rand(4, 1, 2) for _ in range(3)]\n",
    "\n",
    "# Create a target tensor with size (4, 1, 2)\n",
    "target_tensor = torch.rand(4, 1, 2)\n",
    "\n",
    "# Calculate the distances between the target tensor and tensors in the list\n",
    "distances = [torch.norm(target_tensor - t, dim=(1, 2)) for t in tensor_list]\n",
    "\n",
    "# Stack the distances to find the index of the closest tensor\n",
    "stacked_distances = torch.stack(distances)\n",
    "closest_index = torch.argmin(stacked_distances)\n",
    "\n",
    "# Extract the closest tensor\n",
    "closest_tensor = tensor_list[closest_index]\n",
    "\n",
    "print(\"Target tensor:\", target_tensor)\n",
    "print(\"Closest tensor:\", closest_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0.1560, 0.1655, 0.2527, 0.1195]),\n",
       " tensor([0.1908, 0.9152, 0.3194, 0.3827]),\n",
       " tensor([0.3898, 0.9658, 0.2833, 0.2512])]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0.3896, 0.2932, 0.4947, 0.3887]),\n",
       " tensor([0.2660, 0.1295, 0.9180, 0.3591]),\n",
       " tensor([0.5160, 0.7101, 0.3941, 0.5560])]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tensor() takes 1 positional argument but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8651/1068967054.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# 创建表示真实坐标的张量\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrue_coords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# 创建一个包含3次采样的张量列表\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: tensor() takes 1 positional argument but 4 were given"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def euclidean_distance(tensor1, tensor2):\n",
    "    return torch.norm(tensor1 - tensor2, p=2)\n",
    "\n",
    "# 创建表示真实坐标的张量\n",
    "true_coords = torch.tensor([[1,1]],[[2,2]],[[3,3]],[[4,4]])\n",
    "\n",
    "# 创建一个包含3次采样的张量列表\n",
    "sampled_coords_list = [\n",
    "   torch.tensor([[1.1,1.1]],[[21,21]],[[31,31]],[[41,41]]),\n",
    "   torch.tensor([[11,11]],[[2.2,2.2]],[[31,13]],[[41,41]]),\n",
    "   torch.tensor([[11,11]],[[21,21]],[[3.4,3.4]],[[41,14]]),\n",
    "   torch.tensor([[11,11]],[[21,1]],[[13,31]],[[4.5,4.5]])\n",
    "]\n",
    "\n",
    "# 初始化用于存储选择的坐标的张量\n",
    "selected_coords = torch.zeros_like(true_coords)\n",
    "\n",
    "# 对于每个人，找到在所有采样中距离真实坐标最近的采样，并选择该采样\n",
    "for person_index in range(true_coords.size(1)):  # 假设每个样本的维度是(1, 4, 2)\n",
    "    person_true_coord = true_coords[0, person_index, :]\n",
    "    min_distance = float('inf')\n",
    "    nearest_sample_index = -1\n",
    "    \n",
    "    for sample_index, sampled_coords in enumerate(sampled_coords_list):\n",
    "        distance = euclidean_distance(person_true_coord, sampled_coords[0, person_index, :])\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            nearest_sample_index = sample_index\n",
    "    \n",
    "    selected_coords[0, person_index, :] = sampled_coords_list[nearest_sample_index][0, person_index, :]\n",
    "\n",
    "print(selected_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1.1000, 1.1000]],\n",
      "\n",
      "        [[2.2000, 2.2000]],\n",
      "\n",
      "        [[3.4000, 3.4000]],\n",
      "\n",
      "        [[4.5000, 4.5000]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def euclidean_distance(tensor1, tensor2):\n",
    "    return torch.norm(tensor1 - tensor2, p=2)\n",
    "\n",
    "# 创建表示真实坐标的张量\n",
    "true_coords = torch.tensor([\n",
    "    [[1, 1]],\n",
    "    [[2, 2]],\n",
    "    [[3, 3]],\n",
    "    [[4, 4]]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "# 创建一个包含3次采样的张量列表\n",
    "sampled_coords_list = [\n",
    "    torch.tensor([\n",
    "        [[1.1, 1.1]],\n",
    "        [[21, 21]],\n",
    "        [[31, 31]],\n",
    "        [[41, 41]]\n",
    "    ], dtype=torch.float32),\n",
    "    torch.tensor([\n",
    "        [[11, 11]],\n",
    "        [[2.2, 2.2]],\n",
    "        [[31, 13]],\n",
    "        [[41, 41]]\n",
    "    ], dtype=torch.float32),\n",
    "    torch.tensor([\n",
    "        [[11, 11]],\n",
    "        [[21, 21]],\n",
    "        [[3.4, 3.4]],\n",
    "        [[41, 14]]\n",
    "    ], dtype=torch.float32),\n",
    "    torch.tensor([\n",
    "        [[11, 11]],\n",
    "        [[21, 1]],\n",
    "        [[13, 31]],\n",
    "        [[4.5, 4.5]]\n",
    "    ], dtype=torch.float32)\n",
    "]\n",
    "\n",
    "# 初始化用于存储选择的坐标的张量\n",
    "selected_coords = torch.zeros_like(true_coords)\n",
    "\n",
    "# 对于每个人，找到在所有采样中距离真实坐标最近的采样，并选择该采样\n",
    "for person_index in range(true_coords.size(0)):\n",
    "    person_true_coord = true_coords[person_index, :]\n",
    "    min_distance = float('inf')\n",
    "    nearest_sample_index = -1\n",
    "    \n",
    "    for sample_index, sampled_coords in enumerate(sampled_coords_list):\n",
    "        distance = euclidean_distance(person_true_coord, sampled_coords[person_index, :])\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            nearest_sample_index = sample_index\n",
    "    \n",
    "    selected_coords[person_index, :] = sampled_coords_list[nearest_sample_index][person_index, :]\n",
    "\n",
    "print(selected_coords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 创建表示真实坐标的张量\n",
    "true_coords = torch.tensor([\n",
    "    [[1, 1]],\n",
    "    [[2, 2]],\n",
    "    [[3, 3]],\n",
    "    [[4, 4]]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "# 创建一个包含3次采样的张量列表\n",
    "sampled_coords_list = [\n",
    "    torch.tensor([\n",
    "        [[1.1, 1.1]],\n",
    "        [[21, 21]],\n",
    "        [[31, 31]],\n",
    "        [[41, 41]]\n",
    "    ], dtype=torch.float32),\n",
    "    torch.tensor([\n",
    "        [[11, 11]],\n",
    "        [[2.2, 2.2]],\n",
    "        [[31, 13]],\n",
    "        [[41, 41]]\n",
    "    ], dtype=torch.float32),\n",
    "    torch.tensor([\n",
    "        [[11, 11]],\n",
    "        [[21, 21]],\n",
    "        [[3.4, 3.4]],\n",
    "        [[41, 14]]\n",
    "    ], dtype=torch.float32),\n",
    "    torch.tensor([\n",
    "        [[11, 11]],\n",
    "        [[21, 1]],\n",
    "        [[13, 31]],\n",
    "        [[4.5, 4.5]]\n",
    "    ], dtype=torch.float32)\n",
    "]\n",
    "\n",
    "# 转换为合并的张量，以便于进行向量化操作\n",
    "sampled_coords_combined = torch.stack(sampled_coords_list)\n",
    "\n",
    "# 计算每个采样坐标与真实坐标之间的距离\n",
    "distances = torch.norm(sampled_coords_combined - true_coords, dim=2)\n",
    "\n",
    "# 找到最小距离的索引\n",
    "nearest_indices = torch.argmin(distances, dim=0)\n",
    "\n",
    "# 根据最小距离的索引选择最近的采样坐标\n",
    "# selected_coords = sampled_coords_combined[nearest_indices, torch.arange(true_coords.size(0)), :]\n",
    "\n",
    "# print(selected_coords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.1000, 1.1000],\n",
      "        [2.2000, 2.2000],\n",
      "        [3.4000, 3.4000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 创建表示真实坐标的张量\n",
    "true_coords = torch.tensor([\n",
    "    [1, 1],\n",
    "    [2, 2],\n",
    "    [3, 3],\n",
    "    [4, 4]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "# 创建一个包含3次采样的张量列表\n",
    "sampled_coords_list = [\n",
    "    torch.tensor([\n",
    "        [1.1, 1.1],\n",
    "        [2.1, 1.9],\n",
    "        [31, 31],\n",
    "        [41, 41]\n",
    "    ], dtype=torch.float32),\n",
    "    torch.tensor([\n",
    "        [11, 11],\n",
    "        [2.2, 2.2],\n",
    "        [31, 13],\n",
    "        [41, 41]\n",
    "    ], dtype=torch.float32),\n",
    "    torch.tensor([\n",
    "        [11, 11],\n",
    "        [21, 21],\n",
    "        [3.4, 3.4],\n",
    "        [41, 14]\n",
    "    ], dtype=torch.float32),\n",
    "    torch.tensor([\n",
    "        [11, 11],\n",
    "        [21, 1],\n",
    "        [13, 31],\n",
    "        [4.5, 4.5]\n",
    "    ], dtype=torch.float32)\n",
    "]\n",
    "\n",
    "# 转换为合并的张量，以便于进行向量化操作\n",
    "sampled_coords_combined = torch.stack(sampled_coords_list)\n",
    "\n",
    "# 计算每个采样坐标与真实坐标之间的距离\n",
    "distances = torch.norm(sampled_coords_combined - true_coords[:, None, :], dim=2)\n",
    "\n",
    "# 找到最小距离的索引\n",
    "nearest_indices = torch.argmin(distances, dim=1)\n",
    "\n",
    "# 构建索引张量以获取每个最近坐标\n",
    "rows = torch.arange(true_coords.size(0))\n",
    "indices = torch.stack([rows, nearest_indices], dim=1)\n",
    "\n",
    "# 使用索引张量来获取每个最近坐标\n",
    "selected_coords = torch.gather(sampled_coords_combined, 1, indices[:, None, :])\n",
    "\n",
    "print(selected_coords.squeeze())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 1.],\n",
      "         [2., 2.],\n",
      "         [2., 2.],\n",
      "         [2., 2.]],\n",
      "\n",
      "        [[2., 2.],\n",
      "         [2., 2.],\n",
      "         [2., 2.],\n",
      "         [2., 2.]],\n",
      "\n",
      "        [[3., 3.],\n",
      "         [3., 3.],\n",
      "         [3., 3.],\n",
      "         [4., 4.]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 真实坐标\n",
    "true_coords = torch.tensor([\n",
    "    [1, 1],\n",
    "    [2, 2],\n",
    "    [3, 3],\n",
    "    [4, 4]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "# 采样坐标列表\n",
    "sampled_coords_list = [\n",
    "    torch.tensor([\n",
    "        [1.1, 1.1],\n",
    "        [2.1, 1.9],\n",
    "        [31, 31],\n",
    "        [41, 41]\n",
    "    ], dtype=torch.float32),\n",
    "    torch.tensor([\n",
    "        [11, 11],\n",
    "        [2.2, 2.2],\n",
    "        [31, 13],\n",
    "        [41, 41]\n",
    "    ], dtype=torch.float32),\n",
    "    torch.tensor([\n",
    "        [11, 11],\n",
    "        [21, 21],\n",
    "        [3.4, 3.4],\n",
    "        [4.4, 4.4]\n",
    "    ], dtype=torch.float32)\n",
    "]\n",
    "\n",
    "# 将采样坐标列表转换为张量\n",
    "sampled_coords_tensor = torch.stack(sampled_coords_list)\n",
    "\n",
    "# 计算每个采样坐标与真实坐标之间的距离\n",
    "distances = torch.cdist(sampled_coords_tensor, true_coords)\n",
    "\n",
    "# 找到每个采样坐标最接近的真实坐标索引\n",
    "closest_indices = torch.argmin(distances, dim=1)\n",
    "\n",
    "# 使用索引获取最接近的真实坐标\n",
    "closest_coords = torch.stack([true_coords[i] for i in closest_indices])\n",
    "\n",
    "print(closest_coords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 1.],\n",
      "         [2., 2.],\n",
      "         [2., 2.],\n",
      "         [2., 2.]],\n",
      "\n",
      "        [[2., 2.],\n",
      "         [2., 2.],\n",
      "         [2., 2.],\n",
      "         [2., 2.]],\n",
      "\n",
      "        [[3., 3.],\n",
      "         [3., 3.],\n",
      "         [3., 3.],\n",
      "         [4., 4.]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 真实坐标\n",
    "true_coords = torch.tensor([\n",
    "    [1, 1],\n",
    "    [2, 2],\n",
    "    [3, 3],\n",
    "    [4, 4]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "# 采样坐标列表\n",
    "sampled_coords_list = [\n",
    "    torch.tensor([\n",
    "        [1.1, 1.1],\n",
    "        [2.1, 1.9],\n",
    "        [31, 31],\n",
    "        [41, 41]\n",
    "    ], dtype=torch.float32),\n",
    "    torch.tensor([\n",
    "        [11, 11],\n",
    "        [2.2, 2.2],\n",
    "        [31, 13],\n",
    "        [41, 41]\n",
    "    ], dtype=torch.float32),\n",
    "    torch.tensor([\n",
    "        [11, 11],\n",
    "        [21, 21],\n",
    "        [3.4, 3.4],\n",
    "        [4.4, 4.4]\n",
    "    ], dtype=torch.float32)\n",
    "]\n",
    "\n",
    "# 将采样坐标列表转换为张量\n",
    "sampled_coords_tensor = torch.stack(sampled_coords_list)\n",
    "\n",
    "# 计算每个采样坐标与真实坐标之间的距离\n",
    "distances = torch.cdist(sampled_coords_tensor, true_coords)\n",
    "\n",
    "# 找到每个采样坐标最接近的真实坐标索引\n",
    "closest_indices = torch.argmin(distances, dim=1)\n",
    "\n",
    "# 使用索引获取最接近的真实坐标\n",
    "closest_coords = true_coords[closest_indices]\n",
    "\n",
    "print(closest_coords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1414,  1.2728,  2.6870,  4.1012],\n",
       "         [ 1.4213,  0.1414,  1.4213,  2.8320],\n",
       "         [42.4264, 41.0122, 39.5980, 38.1838],\n",
       "         [56.5685, 55.1543, 53.7401, 52.3259]],\n",
       "\n",
       "        [[14.1421, 12.7279, 11.3137,  9.8995],\n",
       "         [ 1.6971,  0.2828,  1.1314,  2.5456],\n",
       "         [32.3110, 31.0161, 29.7321, 28.4605],\n",
       "         [56.5685, 55.1543, 53.7401, 52.3259]],\n",
       "\n",
       "        [[14.1421, 12.7279, 11.3137,  9.8995],\n",
       "         [28.2843, 26.8701, 25.4558, 24.0416],\n",
       "         [ 3.3941,  1.9799,  0.5657,  0.8485],\n",
       "         [ 4.8083,  3.3941,  1.9799,  0.5657]]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 2])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_coords_tensor.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1414,  0.1414, 39.5980, 52.3259],\n",
       "        [14.1421,  0.2828, 29.7321, 52.3259],\n",
       "        [14.1421, 26.8701,  0.5657,  0.5657]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm((sampled_coords_tensor- true_coords),dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices=torch.min(torch.norm((sampled_coords_tensor- true_coords),dim=-1),dim=0)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 2, 2])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=torch.tensor([[0,0], [0,0], [2,2], [2,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_coords_tensor.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Index tensor must have the same number of dimensions as input tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8651/2430540822.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_coords_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Index tensor must have the same number of dimensions as input tensor"
     ]
    }
   ],
   "source": [
    "torch.gather(sampled_coords_tensor,0,index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 2])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_coords_tensor.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.1000,  1.1000],\n",
       "         [ 2.1000,  1.9000],\n",
       "         [31.0000, 31.0000],\n",
       "         [41.0000, 41.0000]],\n",
       "\n",
       "        [[11.0000, 11.0000],\n",
       "         [ 2.2000,  2.2000],\n",
       "         [31.0000, 13.0000],\n",
       "         [41.0000, 41.0000]],\n",
       "\n",
       "        [[11.0000, 11.0000],\n",
       "         [21.0000, 21.0000],\n",
       "         [ 3.4000,  3.4000],\n",
       "         [ 4.4000,  4.4000]]])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_coords_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 2, 2],\n",
       "        [0, 0, 2, 2]])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[0, 0, 2, 2],[0, 0, 2, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Index tensor must have the same number of dimensions as input tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8651/847151798.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_coords_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Index tensor must have the same number of dimensions as input tensor"
     ]
    }
   ],
   "source": [
    "torch.gather(sampled_coords_tensor,1,torch.tensor([[0, 0, 2, 2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Given tensors\n",
    "sampled_coords_tensor = torch.tensor([[[1.1000, 1.1000],\n",
    "                                      [2.1000, 1.9000],\n",
    "                                      [31.0000, 31.0000],\n",
    "                                      [41.0000, 41.0000]],\n",
    "                                     \n",
    "                                     [[11.0000, 11.0000],\n",
    "                                      [2.2000, 2.2000],\n",
    "                                      [31.0000, 13.0000],\n",
    "                                      [41.0000, 41.0000]],\n",
    "                                     \n",
    "                                     [[11.0000, 11.0000],\n",
    "                                      [21.0000, 21.0000],\n",
    "                                      [3.4000, 3.4000],\n",
    "                                      [4.4000, 4.4000]]])\n",
    "target= torch.tensor([[[1.000, 1.000],\n",
    "                                      [2.000, 2.000],\n",
    "                                      [3.0000, 3.0000],\n",
    "                                      [4.0000, 4.0000]]])\n",
    "# index = torch.tensor([0, 0, 2, 2])\n",
    "\n",
    "# # Select coordinates using index\n",
    "# output_coords = sampled_coords_tensor[torch.arange(sampled_coords_tensor.size(0)), index]\n",
    "\n",
    "# print(output_coords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 2])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转换为合并的张量，以便于进行向量化操作\n",
    "# sampled_coords_combined = torch.stack(sampled_coords_tensor)\n",
    "\n",
    "# 计算每个采样坐标与真实坐标之间的距离\n",
    "distances = torch.norm(sampled_coords_tensor - target, dim=2)\n",
    "\n",
    "# 找到最小距离的索引\n",
    "nearest_indices = torch.argmin(distances, dim=0)\n",
    "\n",
    "# # 构建索引张量以获取每个最近坐标\n",
    "rows = torch.arange(target.size(1))\n",
    "# indices = torch.stack([nearest_indices,rows], dim=1)\n",
    "\n",
    "selected_values = sampled_coords_tensor[nearest_indices, rows]\n",
    "\n",
    "selected_tensor = selected_values.view(-1, 2)\n",
    "\n",
    "# # 使用索引张量来获取每个最近坐标\n",
    "# selected_coords = torch.gather(sampled_coords_combined, 1, indices[:, None, :])\n",
    "\n",
    "# print(selected_coords.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1000, 1.1000],\n",
       "        [2.1000, 1.9000],\n",
       "        [3.4000, 3.4000],\n",
       "        [4.4000, 4.4000]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0],\n",
       "        [0, 1],\n",
       "        [2, 2],\n",
       "        [2, 3]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "select() received an invalid combination of arguments - got (Tensor, int, Tensor), but expected one of:\n * (Tensor input, int dim, int index)\n      didn't match because some of the arguments have invalid types: (Tensor, int, !Tensor!)\n * (Tensor input, name dim, int index)\n      didn't match because some of the arguments have invalid types: (Tensor, !int!, !Tensor!)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10231/3399698936.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_coords_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: select() received an invalid combination of arguments - got (Tensor, int, Tensor), but expected one of:\n * (Tensor input, int dim, int index)\n      didn't match because some of the arguments have invalid types: (Tensor, int, !Tensor!)\n * (Tensor input, name dim, int index)\n      didn't match because some of the arguments have invalid types: (Tensor, !int!, !Tensor!)\n"
     ]
    }
   ],
   "source": [
    "torch.select(sampled_coords_tensor, 1,indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.1000, 1.9000])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_coords_tensor[0,1]tensor([[1.1000, 1.1000],[2.1000, 1.9000],[3.4000, 3.4000][4.4000, 4.4000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_values = sampled_coords_tensor[indices[:, 0], indices[:, 1]]\n",
    "\n",
    "selected_tensor = selected_values.view(-1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1000, 1.1000],\n",
       "        [2.1000, 1.9000],\n",
       "        [3.4000, 3.4000],\n",
       "        [4.4000, 4.4000]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.1000, 1.1000])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_coords_tensor[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.4000, 4.4000])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_coords_tensor[2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [2, 2, 2, 3]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # 用来存储每个item的采样概率\n",
    "        sampling_probs = []\n",
    "\n",
    "        # 计算每个item与第一个tensor的采样概率\n",
    "        for coordinates_tensor in sampled_coords_tensor:\n",
    "\n",
    "            # 计算该item在第一个分布下的log概率之和\n",
    "            # log_prob_sum = gauss_distribution.log_prob(coordinates_tensor).sum()\n",
    "            fde=torch.norm((coordinates_tensor.reshape(4, 2) - target.reshape(4, 2)),dim=1)\n",
    "\n",
    "            # 将采样概率存储到列表中\n",
    "            sampling_probs.append(fde)\n",
    "\n",
    "        # 找到具有最大采样概率的item的索引\n",
    "        max_prob_index = torch.argmin(torch.stack(sampling_probs),dim=0)\n",
    "\n",
    "        # # 选取最有可能是由第一个tensor采样得到的item\n",
    "        selected_tensor = sampled_coords_tensor[max_prob_index]\n",
    "        # selected_sampling_probs= sampling_probs[max_prob_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 2, 2])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_prob_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "shape mismatch: indexing tensors could not be broadcast together with shapes [3], [4]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8651/2203623094.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Select coordinates using index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0moutput_coords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msampled_coords_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_coords_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_coords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: shape mismatch: indexing tensors could not be broadcast together with shapes [3], [4]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Given tensors\n",
    "sampled_coords_tensor = torch.tensor([[[1.1000, 1.1000],\n",
    "                                      [2.1000, 1.9000],\n",
    "                                      [31.0000, 31.0000],\n",
    "                                      [41.0000, 41.0000]],\n",
    "                                     \n",
    "                                     [[11.0000, 11.0000],\n",
    "                                      [2.2000, 2.2000],\n",
    "                                      [31.0000, 13.0000],\n",
    "                                      [41.0000, 41.0000]],\n",
    "                                     \n",
    "                                     [[11.0000, 11.0000],\n",
    "                                      [21.0000, 21.0000],\n",
    "                                      [3.4000, 3.4000],\n",
    "                                      [4.4000, 4.4000]]])\n",
    "\n",
    "index = torch.tensor([0, 0, 2, 2])\n",
    "\n",
    "# Select coordinates using index\n",
    "output_coords = sampled_coords_tensor[torch.arange(sampled_coords_tensor.size(0)), index]\n",
    "\n",
    "print(output_coords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tensor() takes 1 positional argument but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8651/3022769352.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m                                       [4.4000, 4.4000]]])\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;31m# Select coordinates using index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0moutput_coords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msampled_coords_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_coords_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: tensor() takes 1 positional argument but 3 were given"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Given tensors\n",
    "sampled_coords_tensor = torch.tensor([[[1.1000, 1.1000],\n",
    "                                      [2.1000, 1.9000],\n",
    "                                      [31.0000, 31.0000],\n",
    "                                      [41.0000, 41.0000]],\n",
    "                                     \n",
    "                                     [[11.0000, 11.0000],\n",
    "                                      [2.2000, 2.2000],\n",
    "                                      [31.0000, 13.0000],\n",
    "                                      [41.0000, 41.0000]],\n",
    "                                     \n",
    "                                     [[11.0000, 11.0000],\n",
    "                                      [21.0000, 21.0000],\n",
    "                                      [3.4000, 3.4000],\n",
    "                                      [4.4000, 4.4000]]])\n",
    "\n",
    "index = torch.tensor([0, 0, 2, 2],[0, 0, 2, 2],[0, 0, 2, 2])\n",
    "# Select coordinates using index\n",
    "output_coords = sampled_coords_tensor[torch.arange(sampled_coords_tensor.size(0)), index]\n",
    "\n",
    "print(output_coords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Index tensor must have the same number of dimensions as input tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8651/3143051255.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_coords_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Index tensor must have the same number of dimensions as input tensor"
     ]
    }
   ],
   "source": [
    "torch.gather(sampled_coords_tensor.permute(1,0,2),0,index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=torch.tensor([0, 0, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.1000,  1.1000],\n",
       "         [ 2.1000,  1.9000],\n",
       "         [31.0000, 31.0000],\n",
       "         [41.0000, 41.0000]],\n",
       "\n",
       "        [[11.0000, 11.0000],\n",
       "         [ 2.2000,  2.2000],\n",
       "         [31.0000, 13.0000],\n",
       "         [41.0000, 41.0000]],\n",
       "\n",
       "        [[11.0000, 11.0000],\n",
       "         [21.0000, 21.0000],\n",
       "         [ 3.4000,  3.4000],\n",
       "         [ 4.4000,  4.4000]]])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_coords_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 2. Expected size 3 but got size 4 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8651/4175294954.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Combine row indices and expanded index tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mcombined_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# Select coordinates using gather\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 2. Expected size 3 but got size 4 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Given tensors\n",
    "sampled_coords_tensor = torch.tensor([[[1.1000, 1.1000],\n",
    "                                      [2.1000, 1.9000],\n",
    "                                      [31.0000, 31.0000],\n",
    "                                      [41.0000, 41.0000]],\n",
    "                                     \n",
    "                                     [[11.0000, 11.0000],\n",
    "                                      [2.2000, 2.2000],\n",
    "                                      [31.0000, 13.0000],\n",
    "                                      [41.0000, 41.0000]],\n",
    "                                     \n",
    "                                     [[11.0000, 11.0000],\n",
    "                                      [21.0000, 21.0000],\n",
    "                                      [3.4000, 3.4000],\n",
    "                                      [4.4000, 4.4000]]])\n",
    "\n",
    "index = torch.tensor([0, 0, 2, 2])\n",
    "\n",
    "# Prepare row indices for gather\n",
    "row_indices = torch.arange(sampled_coords_tensor.size(0)).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "# Expand index tensor to match the shape of sampled_coords_tensor\n",
    "expanded_index = index.unsqueeze(1).unsqueeze(2).expand(-1, sampled_coords_tensor.shape[1], 2)\n",
    "\n",
    "# Combine row indices and expanded index tensor\n",
    "combined_indices = torch.cat([row_indices, expanded_index], dim=2)\n",
    "\n",
    "# Select coordinates using gather\n",
    "output_coords = torch.gather(sampled_coords_tensor, 1, combined_indices)\n",
    "\n",
    "print(output_coords.squeeze())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Index tensor must have the same number of dimensions as input tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8651/3721905495.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrow_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mselected_coords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_coords_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Index tensor must have the same number of dimensions as input tensor"
     ]
    }
   ],
   "source": [
    "    row_indices, col_indices = index[:, 0], index[:, 1]\n",
    "    selected_coords = torch.gather(sampled_coords_tensor, 1, col_indices.view(-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = torch.tensor([0, 0, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_coords_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (2) must match the existing size (4) at non-singleton dimension 2.  Target sizes: [-1, 4, 2].  Tensor sizes: [4]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8651/2417796334.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampled_coords_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (2) must match the existing size (4) at non-singleton dimension 2.  Target sizes: [-1, 4, 2].  Tensor sizes: [4]"
     ]
    }
   ],
   "source": [
    "index.expand(-1, sampled_coords_tensor.shape[1], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (2) must match the existing size (4) at non-singleton dimension 2.  Target sizes: [1, 4, 2].  Tensor sizes: [4]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8651/2754821558.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 将索引张量扩展为合适的形状，以便与 sampled_coords_tensor 匹配\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mexpanded_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampled_coords_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 使用 gather 函数从 sampled_coords_tensor 中收集坐标\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgathered_coords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_coords_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (2) must match the existing size (4) at non-singleton dimension 2.  Target sizes: [1, 4, 2].  Tensor sizes: [4]"
     ]
    }
   ],
   "source": [
    "    # 将索引张量扩展为合适的形状，以便与 sampled_coords_tensor 匹配\n",
    "    expanded_index = index.expand(1, sampled_coords_tensor.shape[1], 2)\n",
    "    \n",
    "    # 使用 gather 函数从 sampled_coords_tensor 中收集坐标\n",
    "    gathered_coords = torch.gather(sampled_coords_tensor, 0, expanded_index)\n",
    "    \n",
    "    # 去除不必要的维度，输出最终的坐标张量\n",
    "    gathered_coords = gathered_coords.squeeze(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 2, 2])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.1000,  1.1000],\n",
       "         [ 2.1000,  1.9000],\n",
       "         [31.0000, 31.0000],\n",
       "         [41.0000, 41.0000]],\n",
       "\n",
       "        [[11.0000, 11.0000],\n",
       "         [ 2.2000,  2.2000],\n",
       "         [31.0000, 13.0000],\n",
       "         [41.0000, 41.0000]],\n",
       "\n",
       "        [[11.0000, 11.0000],\n",
       "         [21.0000, 21.0000],\n",
       "         [ 3.4000,  3.4000],\n",
       "         [ 4.4000,  4.4000]]])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_coords_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 2, 2])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.1000,  1.1000],\n",
       "         [ 2.1000,  1.9000],\n",
       "         [31.0000, 31.0000],\n",
       "         [41.0000, 41.0000]],\n",
       "\n",
       "        [[ 1.1000,  1.1000],\n",
       "         [ 2.1000,  1.9000],\n",
       "         [31.0000, 31.0000],\n",
       "         [41.0000, 41.0000]],\n",
       "\n",
       "        [[11.0000, 11.0000],\n",
       "         [21.0000, 21.0000],\n",
       "         [ 3.4000,  3.4000],\n",
       "         [ 4.4000,  4.4000]],\n",
       "\n",
       "        [[11.0000, 11.0000],\n",
       "         [21.0000, 21.0000],\n",
       "         [ 3.4000,  3.4000],\n",
       "         [ 4.4000,  4.4000]]])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.index_select(sampled_coords_tensor, 0, index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TrafficPredict",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
