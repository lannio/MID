{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "import yaml\n",
    "import argparse\n",
    "import dill\n",
    "import pdb\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils as utils\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions.multivariate_normal as torchdist\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "from torch.utils import data\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Module, Parameter, ModuleList, Linear\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from tqdm.auto import tqdm\n",
    "from easydict import EasyDict\n",
    "\n",
    "import datetime, shutil, argparse, logging, sys\n",
    " \n",
    "from dataset.preprocessing import get_node_timestep_data, collate \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args real\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Pytorch implementation of Rein')\n",
    "    parser.add_argument('--config', default='')\n",
    "    return parser.parse_args()\n",
    "\n",
    "# args = parse_args()\n",
    "# with open(args.config) as f:\n",
    "#     config = yaml.safe_load(f)\n",
    "\n",
    "# for k, v in vars(args).items():\n",
    "#     config[k] = v\n",
    "# config[\"config_name\"] = args.config.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "# config = EasyDict(config)\n",
    "# agent = MID(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args temp\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--run_file',type=str, default='main.py')\n",
    "    parser.add_argument('--config_file',type=str, default='./configs/baseline.yaml')\n",
    "\n",
    "    parser.add_argument('--folder_date',type=str, default='0706')\n",
    "    parser.add_argument('--dataset',type=str, default='eth')\n",
    "    parser.add_argument('--exp',type=str, default='demo')\n",
    "\n",
    "    parser.add_argument(\"--batch_size\", default=256, type=int)\n",
    "    parser.add_argument(\"--eval_batch_size\", default=256, type=int)\n",
    "    parser.add_argument('--shuffle', default=True, type=bool)\n",
    "    parser.add_argument(\"--eval_every\", default=1, type=int)\n",
    "\n",
    "    parser.add_argument(\"--num_steps\", default=100, type=int)\n",
    "    parser.add_argument(\"--num_ddim\", default=10, type=int)\n",
    "    parser.add_argument(\"--ddim_eta\", default=0.0, type=float)\n",
    "    parser.add_argument(\"--clip_denoised\", default=False, type=bool)\n",
    "\n",
    "    parser.add_argument(\"--loss_diffusion_rate\", default=10., type=float)\n",
    "    parser.add_argument(\"--loss_gau_rate\", default=100000., type=float)\n",
    "    parser.add_argument(\"--loss_mean_rate\", default=10., type=float)\n",
    "\n",
    "    parser.add_argument(\"--diffusion_sample_num\", default=1, type=int)\n",
    "    parser.add_argument(\"--point_dim\", default=2, type=int)\n",
    "    parser.add_argument(\"--pred_length\", default=1, type=int)\n",
    "    parser.add_argument(\"--end_list\", default=10, type=int)\n",
    "    parser.add_argument(\"--sample\", default=20, type=int)\n",
    "\n",
    "    parser.add_argument(\"--device\", default=7, type=int)\n",
    "    parser.add_argument('--seed', default=113, type=int)\n",
    "    parser.add_argument(\"--lr\", default=0.001, type=float)\n",
    "    parser.add_argument('--gamma', default=0.95, type=float)\n",
    "    parser.add_argument(\"--epochs\", default=2, type=int)\n",
    "    parser.add_argument(\"--augment\", default=True, type=bool)\n",
    "\n",
    "    parser.add_argument('--data_dir',type=str, default='../../diffusion/MID/processed_data')    \n",
    "    parser.add_argument('--gpu_deterministic', default=False, type=bool, help='set cudnn in deterministic mode (slow)')\n",
    "\n",
    "\n",
    "    return parser.parse_args(args=[])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args 2\n",
    "def get_traj_hypers():\n",
    "    hypers = { \n",
    "    'state':\n",
    "        {'PEDESTRIAN':\n",
    "            {'position': ['x', 'y'],\n",
    "             'velocity': ['x', 'y'],\n",
    "             'acceleration': ['x', 'y']\n",
    "            }\n",
    "        },\n",
    "    'pred_state': {'PEDESTRIAN': {'velocity': ['x', 'y']}},\n",
    "    'edge_encoding': True,\n",
    "    'edge_addition_filter': [0.25, 0.5, 0.75, 1.0],\n",
    "    'edge_removal_filter': [1.0, 0.0],\n",
    "    'dynamic_edges': 'yes',\n",
    "    'incl_robot_node': False,\n",
    "    'node_freq_mult_train': False,\n",
    "    'node_freq_mult_eval': False,\n",
    "    'scene_freq_mult_train': False,\n",
    "    'scene_freq_mult_eval': False,\n",
    "    'scene_freq_mult_viz': False,\n",
    "    'use_map_encoding': False,\n",
    "    }\n",
    "    return hypers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common function\n",
    "def set_gpu(gpu):\n",
    "    torch.cuda.set_device('cuda:{}'.format(gpu))\n",
    "\n",
    "def set_cuda(deterministic=True):\n",
    "    if torch.cuda.is_available():\n",
    "        if not deterministic:\n",
    "            torch.backends.cudnn.deterministic = False\n",
    "            torch.backends.cudnn.benchmark = True\n",
    "        else:\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "            torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def get_output_dir(folder,dataset,exp):\n",
    "    output_dir = os.path.join('/home/yaoliu/scratch/experiment/rein/' + folder, dataset, exp)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    return output_dir\n",
    "\n",
    "def setup_logging(name, output_dir, console=True):\n",
    "    log_format = logging.Formatter(\"%(asctime)s : %(message)s\")\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.handlers = []\n",
    "    output_file = os.path.join(output_dir, 'output.log')\n",
    "    file_handler = logging.FileHandler(output_file)\n",
    "    file_handler.setFormatter(log_format)\n",
    "    logger.addHandler(file_handler)\n",
    "    if console:\n",
    "        console_handler = logging.StreamHandler(sys.stdout)\n",
    "        console_handler.setFormatter(log_format)\n",
    "        logger.addHandler(console_handler)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    return logger\n",
    "\n",
    "def set_seed(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def copy_source(file, output_dir):\n",
    "    shutil.copyfile(file, os.path.join(output_dir, os.path.basename(file)))\n",
    "\n",
    "def restore(data):\n",
    "    \"\"\"\n",
    "    In case we dilled some structures to share between multiple process this function will restore them.\n",
    "    If the data input are not bytes we assume it was not dilled in the first place\n",
    "\n",
    "    :param data: Possibly dilled data structure\n",
    "    :return: Un-dilled data structure\n",
    "    \"\"\"\n",
    "    if type(data) is bytes:\n",
    "        return dill.loads(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data function\n",
    "class EnvironmentDataset(object):\n",
    "    def __init__(self, env, state, pred_state, node_freq_mult, scene_freq_mult, hyperparams, **kwargs):\n",
    "        self.env = env\n",
    "        self.state = state\n",
    "        self.pred_state = pred_state\n",
    "        self.hyperparams = hyperparams\n",
    "        self.max_ht = 7 # 7\n",
    "        self.max_ft = 12\n",
    "        self.node_type_datasets = list() # 1-->1670\n",
    "        self._augment = False\n",
    "        for node_type in env.NodeType:\n",
    "            if node_type not in hyperparams['pred_state']:\n",
    "                continue\n",
    "            self.node_type_datasets.append(NodeTypeDataset(env, node_type, state, pred_state, node_freq_mult,\n",
    "                                                           scene_freq_mult, hyperparams, **kwargs))\n",
    "\n",
    "    @property\n",
    "    def augment(self):\n",
    "        return self._augment\n",
    "\n",
    "    @augment.setter\n",
    "    def augment(self, value):\n",
    "        self._augment = value\n",
    "        for node_type_dataset in self.node_type_datasets:\n",
    "            node_type_dataset.augment = value\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.node_type_datasets)\n",
    "\n",
    "\n",
    "class NodeTypeDataset(data.Dataset):\n",
    "    def __init__(self, env, node_type, state, pred_state, node_freq_mult,\n",
    "                 scene_freq_mult, hyperparams, augment=False, **kwargs):\n",
    "        self.env = env\n",
    "        self.state = state\n",
    "        '''\n",
    "        {'PEDESTRIAN': {'position': ['x', 'y'],\n",
    "        'velocity': ['x', 'y'],\n",
    "        'acceleration': ['x', 'y']}}\n",
    "        '''\n",
    "        self.pred_state = pred_state # {'PEDESTRIAN': {'velocity': ['x', 'y']}}\n",
    "        self.hyperparams = hyperparams\n",
    "        self.max_ht = 7 # 7\n",
    "        self.max_ft = 12 #12\n",
    "\n",
    "        self.augment = augment\n",
    "\n",
    "        self.node_type = node_type # PEDESTRIAN\n",
    "        self.index = self.index_env(node_freq_mult, scene_freq_mult, **kwargs)\n",
    "        self.len = len(self.index) # 1670\n",
    "        self.edge_types = [edge_type for edge_type in env.get_edge_types() if edge_type[0] is node_type] # [(PEDESTRIAN, PEDESTRIAN)]\n",
    "\n",
    "    def index_env(self, node_freq_mult, scene_freq_mult, **kwargs): # False False\n",
    "        index = list()\n",
    "        for scene in self.env.scenes:\n",
    "            present_node_dict = scene.present_nodes(np.arange(0, scene.timesteps), type=self.node_type, **kwargs)\n",
    "            for t, nodes in present_node_dict.items():\n",
    "                for node in nodes:\n",
    "                    index += [(scene, t, node)] *\\\n",
    "                             (scene.frequency_multiplier if scene_freq_mult else 1) *\\\n",
    "                             (node.frequency_multiplier if node_freq_mult else 1)\n",
    "\n",
    "        return index\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        (scene, t, node) = self.index[i]\n",
    "\n",
    "        if self.augment:\n",
    "            scene = scene.augment()\n",
    "            node = scene.get_node_by_id(node.id)\n",
    "\n",
    "        return get_node_timestep_data(self.env, scene, t, node, self.state, self.pred_state,\n",
    "                                      self.edge_types, self.max_ht, self.max_ft, self.hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VarianceSchedule(Module):\n",
    "\n",
    "    def __init__(self, num_steps, mode='linear',beta_1=1e-4, beta_T=5e-2, cosine_s=8e-3):\n",
    "        '''\n",
    "            num_steps=100,\n",
    "            beta_T=5e-2,\n",
    "            mode='linear'\n",
    "        '''\n",
    "        super().__init__()\n",
    "        assert mode in ('linear', 'cosine')\n",
    "        self.num_steps = num_steps # 100\n",
    "\n",
    "        self.beta_1 = beta_1 # 1e-4\n",
    "        self.beta_T = beta_T # 5e-2\n",
    "        self.mode = mode # 'linear'\n",
    "\n",
    "        if mode == 'linear':\n",
    "            betas = torch.linspace(beta_1, beta_T, steps=num_steps)\n",
    "        elif mode == 'cosine':\n",
    "            timesteps = (\n",
    "            torch.arange(num_steps + 1) / num_steps + cosine_s\n",
    "            )\n",
    "            alphas = timesteps / (1 + cosine_s) * math.pi / 2\n",
    "            alphas = torch.cos(alphas).pow(2)\n",
    "            alphas = alphas / alphas[0]\n",
    "            betas = 1 - alphas[1:] / alphas[:-1]\n",
    "            betas = betas.clamp(max=0.999)\n",
    "\n",
    "        betas = torch.cat([torch.zeros([1]), betas], dim=0)     # Padding\n",
    "\n",
    "        alphas = 1 - betas\n",
    "        log_alphas = torch.log(alphas)\n",
    "        for i in range(1, log_alphas.size(0)):  # 1 to T\n",
    "            log_alphas[i] += log_alphas[i - 1]\n",
    "        alpha_bars = log_alphas.exp()\n",
    "\n",
    "        sigmas_flex = torch.sqrt(betas)\n",
    "        sigmas_inflex = torch.zeros_like(sigmas_flex)\n",
    "        for i in range(1, sigmas_flex.size(0)):\n",
    "            sigmas_inflex[i] = ((1 - alpha_bars[i-1]) / (1 - alpha_bars[i])) * betas[i]\n",
    "        sigmas_inflex = torch.sqrt(sigmas_inflex)\n",
    "\n",
    "        self.register_buffer('betas', betas)\n",
    "        self.register_buffer('alphas', alphas)\n",
    "        self.register_buffer('alpha_bars', alpha_bars)\n",
    "        self.register_buffer('sigmas_flex', sigmas_flex)\n",
    "        self.register_buffer('sigmas_inflex', sigmas_inflex)\n",
    "\n",
    "    def uniform_sample_t(self, batch_size):\n",
    "        ts = np.random.choice(np.arange(1, self.num_steps+1), batch_size)\n",
    "        return ts.tolist()\n",
    "\n",
    "    def get_sigmas(self, t, flexibility):\n",
    "        assert 0 <= flexibility and flexibility <= 1\n",
    "        sigmas = self.sigmas_flex[t] * flexibility + self.sigmas_inflex[t] * (1 - flexibility)\n",
    "        return sigmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract(a, t, x_shape):\n",
    "    batch_size = t.shape[0]\n",
    "    out = a.to(t.device).gather(0, t).float()\n",
    "    out = out.reshape(batch_size, *((1,) * (len(x_shape) - 1)))\n",
    "    return out\n",
    "\n",
    "def get_pred_loss(pred, selected_end, gt):\n",
    "    # bs,12,5 bs,1,2, bs,12,2\n",
    "    sx = torch.exp(pred[:, :, 2])  # sx\n",
    "    sy = torch.exp(pred[:, :, 3])  # sy\n",
    "    corr = torch.tanh(pred[:, :, 4])  # corr\n",
    "\n",
    "    cov = torch.zeros(pred.shape[0], pred.shape[1], 2, 2).to('cuda')\n",
    "    cov[:, :, 0, 0] = sx * sx\n",
    "    cov[:, :, 0, 1] = corr * sx * sy\n",
    "    cov[:, :, 1, 0] = corr * sx * sy\n",
    "    cov[:, :, 1, 1] = sy * sy\n",
    "    mean = pred[:, :, 0:2]\n",
    "    mvn = torch.distributions.multivariate_normal.MultivariateNormal(mean, cov)\n",
    "    loss_gt = - mvn.log_prob(gt).sum()\n",
    "    loss_mean = F.mse_loss(mean[:,-1,:].contiguous().view(-1, 2), selected_end.contiguous().view(-1, 2), reduction='mean')\n",
    "    # loss=loss_gt/args.loss_gau_rate + loss_mean/args.loss_mean_rate\n",
    "\n",
    "    return loss_gt, loss_mean\n",
    "\n",
    "def get_pred_de(pred, gt):\n",
    "    predlist=len(pred)\n",
    "    kstep_V_pred_ls = []\n",
    "    # gt = gt.permute(1,0,2)*0.4\n",
    "\n",
    "    sx = torch.exp(pred[:, :, 2])  # sx\n",
    "    sy = torch.exp(pred[:, :, 3])  # sy\n",
    "    corr = torch.tanh(pred[:, :, 4])  # corr\n",
    "\n",
    "    cov = torch.zeros(pred.shape[0], pred.shape[1], 2, 2).to('cuda')\n",
    "    cov[:, :, 0, 0] = sx * sx\n",
    "    cov[:, :, 0, 1] = corr * sx * sy\n",
    "    cov[:, :, 1, 0] = corr * sx * sy\n",
    "    cov[:, :, 1, 1] = sy * sy\n",
    "    mean = pred[:, :, 0:2]\n",
    "    mvn = torch.distributions.multivariate_normal.MultivariateNormal(mean, cov)\n",
    "\n",
    "    KSTEPS=args.sample\n",
    "    for i in range(KSTEPS-1):\n",
    "        kstep_V_pred_ls.append(torch.cumsum((mvn.sample()*0.4), dim=0))  # cat [12, num_person, 2]\n",
    "    kstep_V_pred_ls.append(torch.cumsum(mean*0.4, dim=0))\n",
    "    kstep_V_pred_ls = torch.stack(kstep_V_pred_ls, dim=0) # [KSTEPS, 12, num_person, 2]\n",
    "\n",
    "    # kstep_V_pred = np.concatenate([traj for traj in kstep_V_pred_ls], axis=1) # [12, KSTEPS * num_person, 2]\n",
    "\n",
    "    \"\"\"end of sampling\"\"\"\n",
    "\n",
    "    V_y_rel_to_abs =  torch.cumsum((gt), dim=0) # [12, num_person, 2] speed???)\n",
    "\n",
    "    ade=torch.mean(torch.min(torch.norm((kstep_V_pred_ls - V_y_rel_to_abs),dim=3),dim=0)[0],dim=[0,1])\n",
    "    fde=torch.mean(torch.min(torch.norm((kstep_V_pred_ls - V_y_rel_to_abs)[:,-1,:,:],dim=2),dim=0)[0],dim=[0])\n",
    "    return ade,fde\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_end(gauss_param_tensor, coordinates_list):\n",
    "    gauss_param_tensor = gauss_param_tensor[:,0,:]\n",
    "\n",
    "    sx = torch.exp(gauss_param_tensor[:, 2])  # sx\n",
    "    sy = torch.exp(gauss_param_tensor[:, 3])  # sy\n",
    "    corr = torch.tanh(gauss_param_tensor[:, 4])  # corr\n",
    "    cov = torch.zeros(gauss_param_tensor.shape[0], 2, 2).to('cuda')\n",
    "    cov[:, 0, 0] = sx * sx\n",
    "    cov[:, 0, 1] = corr * sx * sy\n",
    "    cov[:, 1, 0] = corr * sx * sy\n",
    "    cov[:, 1, 1] = sy * sy\n",
    "    mean = gauss_param_tensor[:, 0:2] # bs,1,2\n",
    "    # 创建MultivariateNormal分布对象\n",
    "    gauss_distribution = MultivariateNormal(mean, cov)\n",
    "\n",
    "    # 用来存储每个item的采样概率\n",
    "    sampling_probs = []\n",
    "\n",
    "    # 计算每个item与第一个tensor的采样概率\n",
    "    for coordinates_tensor in coordinates_list:\n",
    "\n",
    "        # 计算该item在第一个分布下的log概率之和\n",
    "        log_prob_sum = gauss_distribution.log_prob(coordinates_tensor).sum()\n",
    "\n",
    "        # 将采样概率存储到列表中\n",
    "        sampling_probs.append(log_prob_sum)\n",
    "\n",
    "    # 找到具有最大采样概率的item的索引\n",
    "    max_prob_index = torch.argmax(torch.tensor(sampling_probs))\n",
    "\n",
    "    # 选取最有可能是由第一个tensor采样得到的item\n",
    "    selected_tensor = coordinates_list[max_prob_index]\n",
    "\n",
    "    return selected_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main args\n",
    "args = parse_args()\n",
    "hyperparams = get_traj_hypers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-19 18:23:57,579 : Namespace(augment=True, batch_size=256, clip_denoised=False, config_file='./configs/baseline.yaml', data_dir='../../diffusion/MID/processed_data', dataset='eth', ddim_eta=0.0, device=7, diffusion_sample_num=1, end_list=10, epochs=2, eval_batch_size=256, eval_every=1, exp='demo', folder_date='0706', gamma=0.95, gpu_deterministic=False, loss_diffusion_rate=10.0, loss_gau_rate=100000.0, loss_mean_rate=10.0, lr=0.001, num_ddim=10, num_steps=100, point_dim=2, pred_length=1, run_file='main.py', sample=20, seed=113, shuffle=True)\n"
     ]
    }
   ],
   "source": [
    "    # main output-log  \n",
    "\n",
    "    output_dir = get_output_dir(args.folder_date, args.dataset, args.exp)\n",
    "    copy_source(args.run_file, output_dir)\n",
    "    copy_source(args.config_file, output_dir)\n",
    "\n",
    "    \n",
    "    set_gpu(args.device)\n",
    "    set_cuda(deterministic=args.gpu_deterministic)\n",
    "    set_seed(args.seed)\n",
    "\n",
    "    logger = setup_logging('job{}'.format(0), output_dir, console=True)\n",
    "    logger.info(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-19 18:23:57,625 : --------build--------\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"--------build--------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-19 18:23:57,698 : ----dataset begin----\n",
      "2023-07-19 18:23:57,699 : train_data_path: ../../diffusion/MID/processed_data/eth_train.pkl\n",
      "2023-07-19 18:23:57,700 : eval_data_path: ../../diffusion/MID/processed_data/eth_test.pkl\n",
      "2023-07-19 18:24:00,487 : ----dataset end----\n"
     ]
    }
   ],
   "source": [
    "# data\n",
    "\n",
    "logger.info(\"----dataset begin----\")\n",
    "train_data_path = osp.join(args.data_dir,args.dataset + \"_train.pkl\")\n",
    "eval_data_path = osp.join(args.data_dir,args.dataset + \"_test.pkl\")\n",
    "logger.info(\"train_data_path: \"+ train_data_path)\n",
    "logger.info(\"eval_data_path: \"+ eval_data_path)\n",
    "\n",
    "train_scenes = []\n",
    "with open(train_data_path, 'rb') as f:\n",
    "    train_env = dill.load(f, encoding='latin1')\n",
    "train_scenes = train_env.scenes\n",
    "train_dataset = EnvironmentDataset(train_env,\n",
    "                                hyperparams['state'],\n",
    "                                hyperparams['pred_state'],\n",
    "                                scene_freq_mult=False,\n",
    "                                node_freq_mult=False,\n",
    "                                hyperparams=hyperparams,\n",
    "                                min_history_timesteps=7,\n",
    "                                min_future_timesteps=12,\n",
    "                                return_robot=True)\n",
    "train_data_loader = dict()\n",
    "for node_type_data_set in train_dataset:\n",
    "    node_type_dataloader = utils.data.DataLoader(node_type_data_set,\n",
    "                                                    collate_fn=collate,\n",
    "                                                    pin_memory = True,\n",
    "                                                    batch_size=args.batch_size,\n",
    "                                                    shuffle=args.shuffle,\n",
    "                                                    num_workers=0,\n",
    "                                                    drop_last=True)\n",
    "    train_data_loader[node_type_data_set.node_type] = node_type_dataloader\n",
    "\n",
    "eval_scenes = []\n",
    "with open(eval_data_path, 'rb') as f:\n",
    "    eval_env = dill.load(f, encoding='latin1')\n",
    "eval_scenes = eval_env.scenes\n",
    "eval_dataset = EnvironmentDataset(eval_env,hyperparams['state'],\n",
    "                                hyperparams['pred_state'],\n",
    "                                scene_freq_mult=False,\n",
    "                                node_freq_mult=False,\n",
    "                                hyperparams=hyperparams,\n",
    "                                min_history_timesteps=7,\n",
    "                                min_future_timesteps=12,\n",
    "                                return_robot=True)\n",
    "eval_data_loader = dict()\n",
    "for node_type_data_set in eval_dataset:\n",
    "    node_type_dataloader = utils.data.DataLoader(node_type_data_set,\n",
    "                                                    collate_fn=collate,\n",
    "                                                    pin_memory=True,\n",
    "                                                    batch_size=args.eval_batch_size,\n",
    "                                                    shuffle=args.shuffle,\n",
    "                                                    num_workers=0,\n",
    "                                                    drop_last=True)\n",
    "    eval_data_loader[node_type_data_set.node_type] = node_type_dataloader\n",
    "\n",
    "logger.info(\"----dataset end----\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-19 18:24:00,604 : ----model begin----\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"----model begin----\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatSquashLinear(Module):\n",
    "    def __init__(self, dim_in, dim_out, dim_ctx):\n",
    "        super(ConcatSquashLinear, self).__init__()\n",
    "        self._layer = Linear(dim_in, dim_out)\n",
    "        self._hyper_bias = Linear(dim_ctx, dim_out, bias=False)\n",
    "        self._hyper_gate = Linear(dim_ctx, dim_out)\n",
    "\n",
    "    def forward(self, ctx, x):\n",
    "        gate = torch.sigmoid(self._hyper_gate(ctx))\n",
    "        bias = self._hyper_bias(ctx)\n",
    "        ret = self._layer(x) * gate + bias\n",
    "\n",
    "        return ret\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[: x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_size=(1024, 512), activation='relu', discrim=False, dropout=-1):\n",
    "        super(MLP, self).__init__()\n",
    "        dims = []\n",
    "        dims.append(input_dim)\n",
    "        dims.extend(hidden_size)\n",
    "        dims.append(output_dim)\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(len(dims)-1):\n",
    "            self.layers.append(nn.Linear(dims[i], dims[i+1]))\n",
    "\n",
    "        if activation == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation = nn.Sigmoid()\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid() if discrim else None\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.layers)):\n",
    "            x = self.layers[i](x)\n",
    "            if i != len(self.layers)-1:\n",
    "                x = self.activation(x)\n",
    "                if self.dropout != -1:\n",
    "                    x = nn.Dropout(min(0.1, self.dropout/3) if i == 1 else self.dropout)(x)\n",
    "            elif self.sigmoid:\n",
    "                x = self.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "class AdditiveAttention(nn.Module):\n",
    "    # Implementing the attention module of Bahdanau et al. 2015 where\n",
    "    # score(h_j, s_(i-1)) = v . tanh(W_1 h_j + W_2 s_(i-1))\n",
    "    def __init__(self, encoder_hidden_state_dim, decoder_hidden_state_dim, internal_dim=None):\n",
    "        super(AdditiveAttention, self).__init__()\n",
    "\n",
    "        if internal_dim is None:\n",
    "            internal_dim = int((encoder_hidden_state_dim + decoder_hidden_state_dim) / 2)\n",
    "\n",
    "        self.w1 = nn.Linear(encoder_hidden_state_dim, internal_dim, bias=False)\n",
    "        self.w2 = nn.Linear(decoder_hidden_state_dim, internal_dim, bias=False)\n",
    "        self.v = nn.Linear(internal_dim, 1, bias=False)\n",
    "\n",
    "    def score(self, encoder_state, decoder_state):\n",
    "        # encoder_state is of shape (batch, enc_dim)\n",
    "        # decoder_state is of shape (batch, dec_dim)\n",
    "        # return value should be of shape (batch, 1)\n",
    "        return self.v(torch.tanh(self.w1(encoder_state) + self.w2(decoder_state)))\n",
    "\n",
    "    def forward(self, encoder_states, decoder_state):\n",
    "        # encoder_states is of shape (batch, num_enc_states, enc_dim)\n",
    "        # decoder_state is of shape (batch, dec_dim)\n",
    "        score_vec = torch.cat([self.score(encoder_states[:, i], decoder_state) for i in range(encoder_states.shape[1])],\n",
    "                              dim=1)\n",
    "        # score_vec is of shape (batch, num_enc_states)\n",
    "\n",
    "        attention_probs = torch.unsqueeze(F.softmax(score_vec, dim=1), dim=2)\n",
    "        # attention_probs is of shape (batch, num_enc_states, 1)\n",
    "\n",
    "        final_context_vec = torch.sum(attention_probs * encoder_states, dim=1)\n",
    "        # final_context_vec is of shape (batch, enc_dim)\n",
    "\n",
    "        return final_context_vec, attention_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_Dim_Up(Module):\n",
    "    def __init__(self):\n",
    "        super(Model_Dim_Up, self).__init__()\n",
    "        # bs, 1, 2 --> bs, 1, 5\n",
    "\n",
    "        self.cnn_up = MLP(input_dim = 2, output_dim = 5, hidden_size=[16,64])\n",
    "        # self.cnn_up = nn.Conv1d(2, 5, 1, padding=0)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x=x.permute(0,2,1)\n",
    "        x = self.cnn_up(x) \n",
    "        # x = x.permute(0,2,1)\n",
    "\n",
    "        return x\n",
    "    \n",
    "# model1=Model_Dim_Up()\n",
    "# data1=torch.randn(64,1,2)\n",
    "# out=model1(data1)\n",
    "# print(out.size())\n",
    "# # torch.Size([64, 1, 5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_Encoder_His(Module):\n",
    "    def __init__(self):\n",
    "        super(Model_Encoder_His, self).__init__()\n",
    "        # bs, time, 6 --> bs, 128\n",
    "\n",
    "        self.encoder_his =nn.LSTM(input_size=6, hidden_size=128, batch_first=True).cuda()\n",
    "        self.dropout = nn.Dropout(p=0.25)\n",
    "\n",
    "\n",
    "    def forward(self, node_history_st):\n",
    "    \n",
    "        his_feat, _ = self.encoder_his(node_history_st)  \n",
    "        his_feat = self.dropout(his_feat)\n",
    "        his_feat = his_feat[:,-1,:]\n",
    "\n",
    "        return his_feat\n",
    "\n",
    "\n",
    "class Model_Encoder_Nei(Module):\n",
    "    def __init__(self):\n",
    "        super(Model_Encoder_Nei, self).__init__()\n",
    "        # bs, time, 6 --> bs, 128\n",
    "\n",
    "        self.encoder_nei =nn.LSTM(input_size=12, hidden_size=128, batch_first=True).cuda()\n",
    "        self.encoder_combine=AdditiveAttention(encoder_hidden_state_dim=128, decoder_hidden_state_dim=128).cuda()\n",
    "        self.dropout = nn.Dropout(p=0.25)\n",
    "\n",
    "        self.state = hyperparams['state']\n",
    "\n",
    "    def forward(self, node_history_st, neighbors, neighbors_edge_value, edge_type, his_feat):\n",
    "    \n",
    "\n",
    "        edge_states_list = list()  \n",
    "        for i, neighbor_states in enumerate(neighbors):  \n",
    "            if len(neighbor_states) == 0:  # There are no neighbors for edge type # TODO necessary?\n",
    "                neighbor_state_length = int(\n",
    "                    np.sum([len(entity_dims) for entity_dims in self.state[edge_type[1]].values()])\n",
    "                ) # 6\n",
    "                edge_states_list.append(torch.zeros(1, 8, neighbor_state_length).cuda())\n",
    "            else:\n",
    "                edge_states_list.append(torch.stack(neighbor_states, dim=0).cuda())\n",
    "        \n",
    "        op_applied_edge_states_list = list()\n",
    "        for neighbors_state in edge_states_list:\n",
    "            op_applied_edge_states_list.append(torch.sum(neighbors_state, dim=0)) #  list of [max_ht, state_dim] torch.Size([8, 6])\n",
    "        combined_neighbors = torch.stack(op_applied_edge_states_list, dim=0) # torch.Size([256, 8, 6])\n",
    "\n",
    "        op_applied_edge_mask_list = list()\n",
    "        for edge_value in neighbors_edge_value:\n",
    "            op_applied_edge_mask_list.append(torch.clamp(torch.sum(edge_value.cuda(), dim=0, keepdim=True), max=1.))\n",
    "        combined_edge_masks = torch.stack(op_applied_edge_mask_list, dim=0) # torch.Size([256, 1])\n",
    "\n",
    "        joint_history = torch.cat([combined_neighbors, node_history_st], dim=-1)\n",
    "\n",
    "        nei_feat, _ = self.encoder_nei(joint_history) \n",
    "        nei_feat = self.dropout(nei_feat)\n",
    "        nei_feat = nei_feat[:,-1,:]\n",
    "\n",
    "        nei_feat = nei_feat * combined_edge_masks\n",
    "\n",
    "        nei_feats = torch.stack([nei_feat], dim=1)\n",
    "\n",
    "        combined_feat, _ = self.encoder_combine(nei_feats, his_feat)\n",
    "        combined_feat = self.dropout(combined_feat)\n",
    "\n",
    "\n",
    "        return combined_feat\n",
    "\n",
    "# model1=Model_Encoder_His().cuda()\n",
    "# model2=Model_Encoder_Nei().cuda()\n",
    "\n",
    "# for node_type, data_loader in train_data_loader.items():\n",
    "#     break\n",
    "# for batch in data_loader:\n",
    "#     break\n",
    "# edge_type=train_env.get_edge_types()[0]\n",
    "# (first_history_index,\n",
    "#     x_t, y_t, x_st_t, y_st_t, # y_t torch.Size([256, 12, 2])\n",
    "#     neighbors_data_st,\n",
    "#     neighbors_edge_value,\n",
    "#     robot_traj_st_t,\n",
    "#     map) = batch\n",
    "\n",
    "# his_f=model1(x_st_t.cuda())\n",
    "# print(his_f.size())\n",
    "# # torch.Size([256, 128])\n",
    "# nei_f=model2(x_st_t.cuda(),restore(neighbors_data_st)[edge_type],restore(neighbors_edge_value)[edge_type],edge_type, his_f)\n",
    "# print(nei_f.size())\n",
    "# # torch.Size([256, 128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_backbone(Module):\n",
    "    def __init__(self):\n",
    "        super(Model_backbone, self).__init__()\n",
    "\n",
    "        context_dim=256\n",
    "        dim=5\n",
    "        self.pos_emb = PositionalEncoding(d_model=2*context_dim, dropout=0.1, max_len=24)\n",
    "        self.concat1 = ConcatSquashLinear(dim,2*context_dim,context_dim+3)\n",
    "        self.layer = nn.TransformerEncoderLayer(d_model=2*context_dim, nhead=4, dim_feedforward=4*context_dim)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.layer, num_layers=3)\n",
    "        self.concat3 = ConcatSquashLinear(2*context_dim,context_dim,context_dim+3)\n",
    "        self.concat4 = ConcatSquashLinear(context_dim,context_dim//2,context_dim+3)\n",
    "        self.linear = ConcatSquashLinear(context_dim//2, dim, context_dim+3)\n",
    "        #self.linear = nn.Linear(128,2)\n",
    "\n",
    "\n",
    "    def forward(self, endpoint_feat, beta, guide):\n",
    "        # bs,1,5 bs,8,256\n",
    "\n",
    "        batch_size = endpoint_feat.size(0)\n",
    "        beta = beta.view(batch_size, 1, 1)          # (B, 1, 1)\n",
    "        guide = guide.view(batch_size, 1, -1)   # (B, 1, F)\n",
    "\n",
    "        time_emb = torch.cat([beta, torch.sin(beta), torch.cos(beta)], dim=-1)  # (B, 1, 3)\n",
    "        ctx_emb = torch.cat([time_emb, guide], dim=-1)    # (B, 1, F+3)\n",
    "        endpoint_feat = self.concat1(ctx_emb,endpoint_feat)\n",
    "        final_emb = endpoint_feat.permute(1,0,2)\n",
    "        final_emb = self.pos_emb(final_emb)\n",
    "\n",
    "\n",
    "        trans = self.transformer_encoder(final_emb).permute(1,0,2)\n",
    "        trans = self.concat3(ctx_emb, trans)\n",
    "        trans = self.concat4(ctx_emb, trans)\n",
    "        return self.linear(ctx_emb, trans)\n",
    "\n",
    "# model1=Model_backbone()\n",
    "# endpoint_feat=torch.randn(64,1,5)\n",
    "# beta=torch.randn(64)\n",
    "# guide=torch.randn(64,256)\n",
    "# model1(endpoint_feat,beta,guide).size()\n",
    "# # torch.Size([64, 1, 5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_his_to_end(Module):\n",
    "    def __init__(self):\n",
    "        super(Model_his_to_end, self).__init__()\n",
    "        # bs, time, 6 --> bs, 1, 2\n",
    "\n",
    "        self.encoder_his = MLP(input_dim = 128, output_dim = 2, hidden_size=[32,8])\n",
    "\n",
    "    def forward(self, his_feat):\n",
    "        end_list=[]\n",
    "\n",
    "        for i in range(args.end_list):\n",
    "            end_feat = self.encoder_his(his_feat)\n",
    "            end_list.append(end_feat)\n",
    "\n",
    "        return end_list\n",
    "    #  10* torch.Size([64,2])\n",
    "# model1=Model_his_to_end()\n",
    "# data1=torch.randn(64,128)\n",
    "# model1(data1)[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_all_to_pred(Module):\n",
    "    def __init__(self):\n",
    "        super(Model_all_to_pred, self).__init__()\n",
    "\n",
    "        context_dim=256\n",
    "\n",
    "        dim=2\n",
    "\n",
    "        self.his_pred = MLP(input_dim = 8, output_dim = 11, hidden_size=[32,128,32])\n",
    "\n",
    "        self.encoder_end = MLP(input_dim = 2, output_dim = 128, hidden_size=[8,32])\n",
    "\n",
    "        self.pos_emb = PositionalEncoding(d_model=2*context_dim, dropout=0.1, max_len=24)\n",
    "        self.concat1 = ConcatSquashLinear(dim,2*context_dim,context_dim)\n",
    "        self.layer = nn.TransformerEncoderLayer(d_model=2*context_dim, nhead=4, dim_feedforward=4*context_dim)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.layer, num_layers=3)\n",
    "        self.concat3 = ConcatSquashLinear(2*context_dim,context_dim,context_dim)\n",
    "        self.concat4 = ConcatSquashLinear(context_dim,context_dim//2,context_dim)\n",
    "        self.linear = ConcatSquashLinear(context_dim//2, 5, context_dim)\n",
    "\n",
    "    def forward(self, his, end, his_feat, nei_feat):\n",
    "        bs=end.size()[0]\n",
    "\n",
    "        his=his[:,:,0:2]\n",
    "        his_p=his.permute(0,2,1)\n",
    "        his_pred=self.his_pred(his_p)\n",
    "        his_pred=his_pred.permute(0,2,1)\n",
    "        his_all=torch.concat((his_pred,end.view(bs, 1, -1)),dim=1) #bs,12,2\n",
    "        ctx_emb=torch.concat((his_feat,nei_feat),dim=1).view(bs, 1, -1)\n",
    "\n",
    "        pred_feat = self.concat1(ctx_emb,his_all)\n",
    "        final_emb = pred_feat.permute(1,0,2)\n",
    "        final_emb = self.pos_emb(final_emb)\n",
    "\n",
    "        trans = self.transformer_encoder(final_emb).permute(1,0,2)\n",
    "        trans = self.concat3(ctx_emb, trans)\n",
    "        trans = self.concat4(ctx_emb, trans)\n",
    "        return self.linear(ctx_emb, trans)\n",
    "\n",
    "# model1=Model_all_to_pred()\n",
    "# his=torch.randn(64,8,2)\n",
    "# end=torch.randn(64,2)\n",
    "# his_feat=torch.randn(64,128)\n",
    "# nei_feat=torch.randn(64,128)\n",
    "# model1(his,end,his_feat,nei_feat).size()\n",
    "# # torch.Size([64, 12, 5])\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_diffusion(Module):\n",
    "    def __init__(self):\n",
    "        super(Model_diffusion, self).__init__()\n",
    "\n",
    "        self.backbone=Model_backbone().cuda()    \n",
    "        self.var_sched = VarianceSchedule(\n",
    "                num_steps=args.num_steps,\n",
    "                beta_T=5e-2,\n",
    "                mode='linear',\n",
    "            ).cuda()    \n",
    "\n",
    "\n",
    "    def get_loss(self, model_up, endpoint, his,nei,mask,edge_type,his_en,nei_en):\n",
    "\n",
    "        endpoint_oir=torch.clone(endpoint) #bs,1,2\n",
    "        endpoint=model_up(endpoint)#bs,1,5\n",
    "\n",
    "\n",
    "        sx = torch.exp(endpoint[:, :, 2])  # sx\n",
    "        sy = torch.exp(endpoint[:, :, 3])  # sy\n",
    "        corr = torch.tanh(endpoint[:, :, 4])  # corr\n",
    "        cov = torch.zeros(endpoint.shape[0], endpoint.shape[1], 2, 2).to('cuda')\n",
    "        cov[:, :, 0, 0] = sx * sx\n",
    "        cov[:, :, 0, 1] = corr * sx * sy\n",
    "        cov[:, :, 1, 0] = corr * sx * sy\n",
    "        cov[:, :, 1, 1] = sy * sy\n",
    "        mean = endpoint[:, :, 0:2] # bs,1,2\n",
    "        mvn = torch.distributions.multivariate_normal.MultivariateNormal(mean, cov)\n",
    "        loss_gau = - mvn.log_prob(endpoint_oir).sum()\n",
    "        loss_mean = F.mse_loss(mean.contiguous().view(-1, 2), endpoint_oir.contiguous().view(-1, 2), reduction='mean')\n",
    "\n",
    "        his_feat=his_en(his)\n",
    "        nei_feat=nei_en(his, nei, mask, edge_type, his_feat)\n",
    "        guide= torch.concat((his_feat,nei_feat),dim=1)\n",
    "\n",
    "        batch_size, _, point_dim = endpoint.size() #$ bs,1,5\n",
    "\n",
    "        t = self.var_sched.uniform_sample_t(batch_size) # 256 t \n",
    "\n",
    "        alpha_bar = self.var_sched.alpha_bars[t]\n",
    "        beta = self.var_sched.betas[t].cuda()\n",
    "\n",
    "        c0 = torch.sqrt(alpha_bar).view(-1, 1, 1).cuda()       # (B, 1, 1)\n",
    "        c1 = torch.sqrt(1 - alpha_bar).view(-1, 1, 1).cuda()   # (B, 1, 1)\n",
    "\n",
    "        e_rand = torch.randn_like(endpoint).cuda()  # (B, N, d) torch.Size([256, 12, 2])\n",
    "\n",
    "        e_theta = self.backbone(c0 * endpoint + c1 * e_rand, beta, guide) # torch.Size([256, 12, 2])\n",
    "\n",
    "        loss_diffusion = F.mse_loss(e_theta.contiguous().view(-1, point_dim), e_rand.contiguous().view(-1, point_dim), reduction='mean')\n",
    "        \n",
    "        return loss_diffusion ,loss_gau ,loss_mean, endpoint, his_feat,nei_feat,guide\n",
    "\n",
    "\n",
    "\n",
    "    def sample(self, model_up, his,nei,mask,edge_type,his_en,nei_en):\n",
    "        gau_up=model_up\n",
    "\n",
    "        traj_list = []\n",
    "        point_dim=args.point_dim\n",
    "        num_points=args.pred_length\n",
    "        self.alphas_cumprod = self.var_sched.alpha_bars\n",
    "\n",
    "        his_feat=his_en(his)\n",
    "        nei_feat=nei_en(his, nei, mask, edge_type, his_feat)\n",
    "        guide= torch.concat((his_feat,nei_feat),dim=1)\n",
    "\n",
    "        for diff_sample_num in range(args.diffusion_sample_num):\n",
    "        \n",
    "            batch_size = guide.size(0)\n",
    "\n",
    "            ddim_timesteps=args.num_ddim\n",
    "            ddim_eta=args.ddim_eta\n",
    "            clip_denoised=args.clip_denoised\n",
    "\n",
    "            c = self.var_sched.num_steps // ddim_timesteps\n",
    "            ddim_timestep_seq = np.asarray(list(range(0, self.var_sched.num_steps, c)))\n",
    "            # add one to get the final alpha values right (the ones from first scale to data during sampling)\n",
    "            ddim_timestep_seq = ddim_timestep_seq + 1\n",
    "            # previous sequence\n",
    "            ddim_timestep_prev_seq = np.append(np.array([0]), ddim_timestep_seq[:-1])\n",
    "\n",
    "\n",
    "\n",
    "            sample_img = torch.randn([batch_size, num_points, point_dim]).to(guide.device)\n",
    "            sample_img=gau_up(sample_img)\n",
    "\n",
    "\n",
    "            ddim_timesteps_test = ddim_timesteps\n",
    "            # ddim_timesteps_test = self.config.ddim_timesteps_test\n",
    "            for i in reversed(range(0, ddim_timesteps_test)) :\n",
    "                t = torch.full((batch_size,), ddim_timestep_seq[i], device=guide.device, dtype=torch.long)\n",
    "                prev_t = torch.full((batch_size,), ddim_timestep_prev_seq[i], device=guide.device, dtype=torch.long)\n",
    "                \n",
    "                # 1. get current and previous alpha_cumprod\n",
    "                \n",
    "                alpha_cumprod_t = extract(self.alphas_cumprod, t, sample_img.shape)\n",
    "                alpha_cumprod_t_prev = extract(self.alphas_cumprod, prev_t, sample_img.shape)\n",
    "        \n",
    "                # 2. predict noise using model\n",
    "                beta = self.var_sched.betas[[t[0].item()]*batch_size]\n",
    "                pred_noise = self.backbone(sample_img, beta, guide)\n",
    "                \n",
    "                # 3. get the predicted x_0\n",
    "                pred_x0 = (sample_img - torch.sqrt((1. - alpha_cumprod_t)) * pred_noise) / torch.sqrt(alpha_cumprod_t)\n",
    "                if clip_denoised:\n",
    "                    pred_x0 = torch.clamp(pred_x0, min=-1., max=1.)\n",
    "                \n",
    "                # 4. compute variance: \"sigma_t(η)\" -> see formula (16)\n",
    "                # σ_t = sqrt((1 − α_t−1)/(1 − α_t)) * sqrt(1 − α_t/α_t−1)\n",
    "                sigmas_t = ddim_eta * torch.sqrt(\n",
    "                    (1 - alpha_cumprod_t_prev) / (1 - alpha_cumprod_t) * (1 - alpha_cumprod_t / alpha_cumprod_t_prev))\n",
    "                \n",
    "                # 5. compute \"direction pointing to x_t\" of formula (12)\n",
    "                pred_dir_xt = torch.sqrt(1 - alpha_cumprod_t_prev - sigmas_t**2) * pred_noise\n",
    "                \n",
    "                # 6. compute x_{t-1} of formula (12)\n",
    "                x_prev = torch.sqrt(alpha_cumprod_t_prev) * pred_x0 + pred_dir_xt + sigmas_t * torch.randn_like(sample_img)\n",
    "\n",
    "                sample_img = x_prev.detach()\n",
    "            traj_list.append(sample_img)\n",
    "\n",
    "        return traj_list,his_feat,nei_feat,guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUR():\n",
    "    def __init__(self):\n",
    "        super(OUR, self).__init__()\n",
    "        self.gau_up=Model_Dim_Up().cuda() \n",
    "        self.his_en=Model_Encoder_His().cuda() \n",
    "        self.nei_en=Model_Encoder_Nei().cuda() \n",
    "        self.model_diffuion=Model_diffusion().cuda() \n",
    "\n",
    "        self.model_end=Model_his_to_end().cuda()\n",
    "        self.model_pred=Model_all_to_pred().cuda()\n",
    "\n",
    "        self.train_dataset=train_dataset\n",
    "        self.hyperparams = hyperparams\n",
    "        \n",
    "\n",
    "        self.optimizer_right = optim.Adam([{'params': self.gau_up.parameters()},\n",
    "                                     {'params': self.model_diffuion.parameters()},\n",
    "                                     {'params': self.his_en.parameters()},\n",
    "                                     {'params': self.nei_en.parameters()},\n",
    "                                    ],\n",
    "                                    lr=args.lr)\n",
    "        self.scheduler_right = optim.lr_scheduler.ExponentialLR(self.optimizer_right,gamma=args.gamma)\n",
    "\n",
    "        self.optimizer_left = optim.Adam([{'params': self.model_end.parameters()},\n",
    "                                     {'params': self.model_pred.parameters()}\n",
    "                                    ],\n",
    "                                    lr=args.lr)\n",
    "        self.scheduler_left = optim.lr_scheduler.ExponentialLR(self.optimizer_left,gamma=args.gamma)\n",
    "        \n",
    "    def train(self):\n",
    "        self.gau_up.train()\n",
    "        self.his_en.train()\n",
    "        self.nei_en.train()\n",
    "        self.model_diffuion.train()\n",
    "\n",
    "        self.model_end.train()\n",
    "        self.model_pred.train()\n",
    "\n",
    "        ade_final=9999\n",
    "        fde_final=9999\n",
    "        ade = 999\n",
    "        fde = 999\n",
    "        ade_epoch=0\n",
    "        fde_epoch=0\n",
    "        ftimesum=0.\n",
    "        btimesum=0.\n",
    "        sample_count=0\n",
    "\n",
    "        for epoch in range(1, args.epochs + 1):\n",
    "            start_time_f = time.time()\n",
    "            self.train_dataset.augment = args.augment\n",
    "            for node_type, data_loader in train_data_loader.items():\n",
    "                pbar = tqdm(data_loader, ncols=80)\n",
    "                right_loss=0.0\n",
    "                right_loss_diff=0.0\n",
    "                right_loss_gau=0.0\n",
    "                right_loss_mean=0.0\n",
    "\n",
    "                left_loss=0.0\n",
    "                left_loss_gau=0.0\n",
    "                left_loss_mean=0.0\n",
    "\n",
    "                count=0\n",
    "                for batch in pbar:\n",
    "                    edge_type=train_env.get_edge_types()[0]\n",
    "                    (first_history_index,\n",
    "                        x_t, y_t, x_st_t, y_st_t, # y_t torch.Size([256, 12, 2])\n",
    "                        neighbors_data_st,\n",
    "                        neighbors_edge_value,\n",
    "                        robot_traj_st_t,\n",
    "                        map) = batch\n",
    "\n",
    "                    self.his=x_st_t.cuda()\n",
    "                    self.gt=y_st_t.cuda()\n",
    "                    self.end=self.gt[:,11:12,:]\n",
    "                    self.nei=restore(neighbors_data_st)[edge_type]\n",
    "                    self.nei_mask=restore(neighbors_edge_value)[edge_type]\n",
    "\n",
    "\n",
    "                    self.optimizer_right.zero_grad()\n",
    "                    right_loss1,right_loss2,right_loss3, end_feat, his_feat,nei_feat,guide = self.model_diffuion.get_loss(self.gau_up, self.end, self.his,self.nei, self.nei_mask,edge_type,self.his_en,self.nei_en)\n",
    "                    train_loss_right = right_loss1*args.loss_diffusion_rate+right_loss2/args.loss_gau_rate+right_loss3*args.loss_mean_rate\n",
    "                    pbar.set_description(f\"Epoch {epoch}, {node_type} Right-MSE: {train_loss_right.item():.2f}\")\n",
    "                    count = count+1\n",
    "                    right_loss = right_loss + train_loss_right.item()\n",
    "                    right_loss_diff = right_loss_diff + right_loss1.item()\n",
    "                    right_loss_gau = right_loss_gau + right_loss2.item()\n",
    "                    right_loss_mean = right_loss_mean + right_loss3.item()\n",
    "                    train_loss_right.backward(retain_graph=True)\n",
    "                    self.optimizer_right.step()\n",
    "\n",
    "\n",
    "                    self.optimizer_left.zero_grad()\n",
    "\n",
    "\n",
    "                    end_list=self.model_end(his_feat)\n",
    "                    # random_index = np.random.randint(0, args.end_list)\n",
    "                    # selected_end = end_list[random_index]\n",
    "                    # print(end_feat.size())\n",
    "                    # print(end_list[0].size())\n",
    "                    selected_end=find_end(end_feat, end_list)\n",
    "                    \n",
    "                    pred=self.model_pred(self.his, selected_end, his_feat, nei_feat)\n",
    "                    left_loss1, left_loss2 = get_pred_loss(pred, selected_end, self.gt)\n",
    "                    train_loss_left= left_loss1/args.loss_gau_rate+left_loss2*args.loss_mean_rate\n",
    "                    pbar.set_description(f\"Epoch {epoch}, {node_type} Left-MSE: {train_loss_left.item():.2f}\")\n",
    "                    left_loss = left_loss + train_loss_left.item()\n",
    "                    left_loss_gau = left_loss_gau + left_loss1.item()\n",
    "                    left_loss_mean = left_loss_mean + left_loss2.item()\n",
    "                    train_loss_left.backward\n",
    "                    self.optimizer_left.step()\n",
    "                    break\n",
    "\n",
    "            end_time_f = time.time()\n",
    "            ftime= end_time_f - start_time_f\n",
    "            ftimesum = ftimesum+ftime\n",
    "            logger.info(f\"Epoch {epoch}, {node_type} Right-MSE: {(right_loss/count):.2f}, loss1 MSE: {(right_loss_diff/count):.2f}, loss2 MSE: {(right_loss_gau/count):.2f}, loss3 MSE: {(right_loss_mean/count):.2f}, train_time: {(ftime):.2f}, train_time_avg: {(ftimesum/epoch):.2f}\")\n",
    "            logger.info(f\"Epoch {epoch}, {node_type} Left-MSE: {(left_loss/count):.2f}, loss2 MSE: {(left_loss_gau/count):.2f}, loss3 MSE: {(left_loss_mean/count):.2f}, train_time: {(ftime):.2f}, train_time_avg: {(ftimesum/epoch):.2f}\")\n",
    "\n",
    "            self.train_dataset.augment = False\n",
    "            if ((epoch % args.eval_every == 0) and (epoch > 0)) or epoch==1:\n",
    "                start_time_b = time.time()\n",
    "                self.gau_up.eval()\n",
    "                self.his_en.eval()\n",
    "                self.nei_en.eval()\n",
    "                self.model_diffuion.eval()\n",
    "\n",
    "                self.model_end.eval()\n",
    "                self.model_pred.eval()\n",
    "\n",
    "                node_type = \"PEDESTRIAN\"\n",
    "\n",
    "                ade_sum=0.0\n",
    "                fde_sum=0.0\n",
    "                test_count=0\n",
    " \n",
    "                for node_type_test, data_loader_test in eval_data_loader.items():\n",
    "                    pbar2 = tqdm(data_loader_test, ncols=80)\n",
    "                    for test_batch in pbar2:\n",
    "                        (first_history_index,\n",
    "                            x_t, y_t, x_st_t, y_st_t, # y_t torch.Size([256, 12, 2])\n",
    "                            neighbors_data_st,\n",
    "                            neighbors_edge_value,\n",
    "                            robot_traj_st_t,\n",
    "                            map) = test_batch\n",
    "\n",
    "                        self.test_his=x_st_t.cuda()\n",
    "                        self.test_gt=y_st_t.cuda()\n",
    "                        self.test_end=self.gt[:,11:12,:]\n",
    "                        self.test_nei=restore(neighbors_data_st)[edge_type]\n",
    "                        self.test_nei_mask=restore(neighbors_edge_value)[edge_type]\n",
    "\n",
    "                        traj_pred_list,his_feat,nei_feat,guide = self.model_diffuion.sample(self.gau_up, self.test_his,self.test_nei,self.test_nei_mask,edge_type,self.his_en,self.nei_en) # bs,1,5\n",
    "                        traj_pred=traj_pred_list[0]\n",
    "                        \n",
    "                        end_list=self.model_end(his_feat)\n",
    "                        # random_index = np.random.randint(0, args.end_list)\n",
    "                        # selected_end = end_list[random_index]\n",
    "                        selected_end=find_end(traj_pred, end_list)\n",
    "                        pred=self.model_pred(self.test_his, selected_end, his_feat, nei_feat)\n",
    "                        ade, fde = get_pred_de(pred, self.test_gt)\n",
    "                        \n",
    "                        ade_sum=ade_sum+ade\n",
    "                        fde_sum=fde_sum+fde\n",
    "                        test_count=test_count+1\n",
    "                        break\n",
    "\n",
    "                    ade=ade_sum/test_count\n",
    "                    fde=fde_sum/test_count\n",
    "\n",
    "\n",
    "\n",
    "                if args.dataset == \"eth\":\n",
    "                    ade = ade/0.6\n",
    "                    fde = fde/0.6\n",
    "                elif args.dataset == \"sdd\":\n",
    "                    ade = ade * 50\n",
    "                    fde = fde * 50\n",
    "\n",
    "                end_time_b = time.time()\n",
    "                btime= end_time_b - start_time_b\n",
    "                btimesum = btimesum+btime\n",
    "                sample_count=sample_count+1\n",
    "\n",
    "                \n",
    "                logger.info(f\"{args.folder_date} {args.dataset}  {args.exp}  :Best of 20: Epoch {epoch} (Train) ADE: {ade} FDE: {fde}, sample_time: {(btime):.2f}, sample_time_avg: {(btimesum/sample_count):.2f}\")\n",
    "                self.gau_up.train()\n",
    "                self.his_en.train()\n",
    "                self.nei_en.train()\n",
    "                self.model_diffuion.train()\n",
    "\n",
    "                self.model_end.train()\n",
    "                self.model_pred.train()\n",
    "        \n",
    "            if (ade_final>ade):\n",
    "                ade_final = ade\n",
    "                ade_epoch = epoch\n",
    "            if (fde_final>fde):\n",
    "                fde_final=fde\n",
    "                fde_epoch = epoch\n",
    "        print(f\"######## Best Of 20 (Train): ADE: {ade_epoch} -- {ade_final} FDE: {fde_epoch} -- {fde_final}\")\n",
    "        logger.info(f\"######## Best Of 20 (Train): ADE: {ade_epoch} -- {ade_final} FDE: {fde_epoch} -- {fde_final}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = OUR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c6b08c45e314eeea97eef0b5967d7de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                   | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-19 18:24:06,880 : Epoch 1, PEDESTRIAN Right-MSE: 11.20, loss1 MSE: 1.03, loss2 MSE: 446.32, loss3 MSE: 0.09, train_time: 5.40, train_time_avg: 5.40\n",
      "2023-07-19 18:24:06,881 : Epoch 1, PEDESTRIAN Left-MSE: 1.21, loss2 MSE: 6222.32, loss3 MSE: 0.12, train_time: 5.40, train_time_avg: 5.40\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef65cf8b1ec14b228d5a6ee2a2c8467a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                     | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-19 18:24:09,942 : 0706 eth  demo  :Best of 20: Epoch 1 (Train) ADE: 11.0094575881958 FDE: 30.555374145507812, sample_time: 3.06, sample_time_avg: 3.06\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5956b7f2ebc4a61b41ee7e84c77079c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                   | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-19 18:24:15,141 : Epoch 2, PEDESTRIAN Right-MSE: 10.96, loss1 MSE: 1.01, loss2 MSE: 439.88, loss3 MSE: 0.08, train_time: 5.20, train_time_avg: 5.30\n",
      "2023-07-19 18:24:15,141 : Epoch 2, PEDESTRIAN Left-MSE: 1.18, loss2 MSE: 6240.21, loss3 MSE: 0.11, train_time: 5.20, train_time_avg: 5.30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59c7ae1cbf5e4355a38d4f5509427d15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                     | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-19 18:24:18,105 : 0706 eth  demo  :Best of 20: Epoch 2 (Train) ADE: 10.58918571472168 FDE: 14.74039077758789, sample_time: 2.96, sample_time_avg: 3.01\n",
      "######## Best Of 20 (Train): ADE: 2 -- 10.58918571472168 FDE: 2 -- 14.74039077758789\n",
      "2023-07-19 18:24:18,109 : ######## Best Of 20 (Train): ADE: 2 -- 10.58918571472168 FDE: 2 -- 14.74039077758789\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。请查看单元格中的代码，以确定故障的可能原因。有关详细信息，请单击 <a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>。有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "agent.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TrafficPredict",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
